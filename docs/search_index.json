[["index.html", "Source code for Himalayan birds shift elevations more to remain within narrow thermal regimes across seasons Section 1 Introduction 1.1 Data processing 1.2 Attribution 1.3 Data access", " Source code for Himalayan birds shift elevations more to remain within narrow thermal regimes across seasons Tarun Menon Vijay Ramesh Sahas Barve Last compiled on 12 September, 2023 Section 1 Introduction This is the readable version containing analysis that models the extent of elevational migration in Himalayan birds using community science data (eBird). 1.1 Data processing The data processing for this project is described in the following sections. Navigate through them using the links in the sidebar. 1.2 Attribution Please contact the following in case of interest in the project. Vijay Ramesh (repo maintainer) Postdoctoral Fellow, Cornell Lab of Ornithology 1.3 Data access The data used in this work is archived on Zenodo. "],["spatial-thinning-of-occurrence-data.html", "Section 2 Spatial thinning of occurrence data 2.1 Load necessary libraries 2.2 Load data", " Section 2 Spatial thinning of occurrence data In this script, we spatially thin occurrence data for creating Figure 1. 2.1 Load necessary libraries Code library(sf) library(dplyr) library(spThin) 2.2 Load data Code loc &lt;- read.csv(&quot;data/localities-for-map.csv&quot;) head(loc) loc_unique &lt;- loc %&gt;% distinct() loc_unique$species &lt;- &quot;species&quot; # carry out spatial thinning with a minimum distance of 15km between records thinned &lt;- thin(loc_unique,lat.col=&quot;LATITUDE&quot;,long.col = &quot;LONGITUDE&quot;, spec.col = &quot;species&quot;,thin.par = 15,reps=1, write.files=T, out.dir=&quot;data/&quot;, out.base=&quot;localities-thinned&quot;) # reload spatially thinned file thin_file &lt;- read.csv(&quot;results/localities-for-map-thinned.csv&quot;) thin_file &lt;- thin_file %&gt;% mutate(region = case_when(LONGITUDE &lt; 83 ~ &quot;west&quot;, LONGITUDE &gt; 83 ~ &quot;east&quot;)) shp &lt;- st_as_sf(thin_file, coords = c(&quot;LONGITUDE&quot;,&quot;LATITUDE&quot;), crs=4326) str(shp) st_write(shp[shp$region==&quot;east&quot;,],&quot;data/shapefiles/localities-east.shp&quot;, driver=&quot;ESRI Shapefile&quot;) st_write(shp[shp$region==&quot;west&quot;,],&quot;data/shapefiles/localities-west.shp&quot;, driver=&quot;ESRI Shapefile&quot;) "],["temperature-elevation-associations.html", "Section 3 Temperature-elevation associations 3.1 Load necessary libraries 3.2 Load shapefiles 3.3 Prepare elevation rasters 3.4 Prepare climate rasters 3.5 Extracting data across elevational bands 3.6 Plot temperature as a function of elevation", " Section 3 Temperature-elevation associations In this script, we calculate temperatures across elevational bands in the Eastern and Western Himalayas. 3.1 Load necessary libraries Code library(raster) library(stringi) library(glue) library(gdalUtils) library(purrr) library(dplyr) library(tidyr) library(scales) library(ggplot2) library(ggthemes) library(sf) library(mapview) library(rgeos) # get ci func ci &lt;- function(x){qnorm(0.975)*sd(x, na.rm = T)/sqrt(length(x))} # prep mode function to aggregate funcMode &lt;- function(x, na.rm = T) { ux &lt;- unique(x) ux[which.max(tabulate(match(x, ux)))] } # a basic test assertthat::assert_that(funcMode(c(2,2,2,2,3,3,3,4)) == as.character(2), msg = &quot;problem in the mode function&quot;) # works 3.2 Load shapefiles Code # Load shapefiles # Please note the below shapefile consists of multiple polygons that combines the east and west himalayas &lt;- st_read(&quot;data/shapefiles/shapefile_himalaya.shp&quot;) mapview(himalayas) # Need to merge a few polygons to essentially split himalayas into the west and the east # This will have to be done at 83E for Nepal # Merge polygons for western himalayas west_toMerge &lt;- himalayas[c(3,4,7,8,9,10),] westHim &lt;- st_crop(west_toMerge,xmin=68.03321,ymin=23.69771,xmax=83,ymax=37.07761) # Merge polygons for eastern himalayas east_toMerge &lt;- himalayas[c(1,2,5,6,10),] east_toMerge &lt;- st_buffer(east_toMerge, dist=0) eastHim &lt;- st_crop(east_toMerge,xmin=83,ymin=25.96462,xmax=97.4115,ymax=30.44728) 3.3 Prepare elevation rasters Code # load elevation and crop to hills size, then mask by hills # Please note that this file is large and is not uploaded to GitHub and can be downloaded from SRTM (Farr et al. 2007) alt &lt;- raster(&quot;data/elevation/alt&quot;) alt.east &lt;- crop(alt, as(eastHim, &quot;Spatial&quot;)) alt.west &lt;- crop(alt, as(westHim, &quot;Spatial&quot;)) rm(alt); gc() # get slope and aspect slopeEast &lt;- terrain(x = alt.east, opt = c(&quot;slope&quot;, &quot;aspect&quot;)) slopeWest &lt;- terrain(x = alt.west, opt = c(&quot;slope&quot;, &quot;aspect&quot;)) # stack rasters elevEast &lt;- raster::stack(alt.east, slopeEast) elevWest &lt;- raster::stack(alt.west, slopeWest) rm(alt.east,alt.west); gc() 3.4 Prepare climate rasters Code # list chelsa files # CHELSA files are not uploaded to GitHub as they are extremely large and can be downloaded from https://chelsa-climate.org/ # Please note that we downloaded four rasters corresponding to minimum and maximum temperatures for the months of January and June chelsaFiles &lt;- list.files(&quot;data/chelsa/&quot;, full.names = TRUE, pattern = &quot;*.tif&quot;) # gather chelsa data over the east chelsaEast &lt;- purrr::map(chelsaFiles, function(chr){ a &lt;- raster(chr) crs(a) &lt;- crs(elevEast) a &lt;- crop(a, as(eastHim, &quot;Spatial&quot;)) return(a) }) # gather chelsa data over the west chelsaWest &lt;- purrr::map(chelsaFiles, function(chr){ a &lt;- raster(chr) crs(a) &lt;- crs(elevWest) a &lt;- crop(a, as(westHim, &quot;Spatial&quot;)) return(a) }) # Divide temperature values by 10 for east chelsaEast[[1]] &lt;- chelsaEast[[1]]/10 chelsaEast[[2]] &lt;- chelsaEast[[2]]/10 chelsaEast[[3]] &lt;- chelsaEast[[3]]/10 chelsaEast[[4]] &lt;- chelsaEast[[4]]/10 # Divide temperature values by 10 for the west chelsaWest[[1]] &lt;- chelsaWest[[1]]/10 chelsaWest[[2]] &lt;- chelsaWest[[2]]/10 chelsaWest[[3]] &lt;- chelsaWest[[3]]/10 chelsaWest[[4]] &lt;- chelsaWest[[4]]/10 # stack chelsa data for the east and west chelsaEast &lt;- raster::stack(chelsaEast) chelsaWest &lt;- raster::stack(chelsaWest) ### Stack prepared rasters # stack rasters for efficient reprojection later env_east &lt;- stack(elevEast, chelsaEast) env_west &lt;- stack(elevWest, chelsaWest) # get proper names elev_names &lt;- c(&quot;elev&quot;, &quot;slope&quot;, &quot;aspect&quot;) chelsa_names &lt;- c(&quot;maxTemp_Jan&quot;, &quot;maxTemp_June&quot;,&quot;minTemp_Jan&quot;,&quot;minTemp_June&quot;) names(env_east) &lt;- as.character(glue(&#39;{c(elev_names, chelsa_names)}&#39;)) names(env_west) &lt;- as.character(glue(&#39;{c(elev_names, chelsa_names)}&#39;)) 3.5 Extracting data across elevational bands Code # make duplicate stack envEast &lt;- env_east[[c(&quot;elev&quot;, chelsa_names)]] envWest &lt;- env_West[[c(&quot;elev&quot;, chelsa_names)]] # convert to list envEast &lt;- as.list(envEast) envWest &lt;- as.list(envWest) # map get values over the stack envEast &lt;- purrr::map(envEast, getValues) envWest &lt;- purrr::map(envWest, getValues) names(envEast) &lt;- c(&quot;elev&quot;, chelsa_names) names(envWest) &lt;- c(&quot;elev&quot;, chelsa_names) # convert to dataframe and round to a particular elevational band you need envEast &lt;- bind_cols(envEast) envWest &lt;- bind_cols(envWest) envEast &lt;- drop_na(envEast) %&gt;% mutate(elev_round = plyr::round_any(elev, 100)) %&gt;% # changed to 100 m intervals dplyr::select(-elev) %&gt;% group_by(elev_round) %&gt;% summarise_all(.funs = list(~mean(.), ~ci(.))) %&gt;% mutate(tempRange_Jan = (maxTemp_Jan_mean - minTemp_Jan_mean), tempRange_June = (maxTemp_June_mean - minTemp_June_mean)) envWest &lt;- drop_na(envWest) %&gt;% mutate(elev_round = plyr::round_any(elev, 100)) %&gt;% # changed to 100 m intervals dplyr::select(-elev) %&gt;% group_by(elev_round) %&gt;% summarise_all(.funs = list(~mean(.), ~ci(.))) %&gt;% mutate(tempRange_Jan = (maxTemp_Jan_mean - minTemp_Jan_mean), tempRange_June = (maxTemp_June_mean - minTemp_June_mean)) # Write results to a .csv west_data &lt;- write.csv(env,&quot;results/westHim_100.csv&quot;, row.names = F) east_data &lt;- write.csv(env,&quot;results/eastHim_100.csv&quot;, row.names = F) 3.6 Plot temperature as a function of elevation Code # eastern Himalayas fig_climate_elevEast &lt;- ggplot(envEast)+ geom_line(aes(x = elev_round, y = mean), size = 0.2, col = &quot;grey&quot;)+ geom_pointrange(aes(x = elev_round, y = mean, ymin=mean-ci, ymax=mean+ci), size = 0.3)+ scale_x_continuous(labels = scales::comma)+ scale_y_continuous(labels = scales::comma)+ facet_wrap(~clim_var, scales = &quot;free_y&quot;)+ theme_few()+ labs(x = &quot;elevation (m) at 100m intervals&quot;, y = &quot;CHELSA variable value&quot;)+ theme(axis.title = element_text(size = 16, face = &quot;bold&quot;), axis.ticks.length.x = unit(.5, &quot;cm&quot;), axis.text = element_text(size = 14), legend.title = element_blank(), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(size = 12)) # save ggplots accordingly ggsave(fig_climate_elevEast, filename = &quot;figs/fig_eastHim_elev100.png&quot;, height = 10, width = 14, device = png(), dpi = 300, units=&quot;in&quot;); dev.off() # western Himalayas fig_climate_elevWest &lt;- ggplot(envWest)+ geom_line(aes(x = elev_round, y = mean), size = 0.2, col = &quot;grey&quot;)+ geom_pointrange(aes(x = elev_round, y = mean, ymin=mean-ci, ymax=mean+ci), size = 0.3)+ scale_x_continuous(labels = scales::comma)+ scale_y_continuous(labels = scales::comma)+ facet_wrap(~clim_var, scales = &quot;free_y&quot;)+ theme_few()+ labs(x = &quot;elevation (m) at 100m intervals&quot;, y = &quot;CHELSA variable value&quot;)+ theme(axis.title = element_text(size = 16, face = &quot;bold&quot;), axis.ticks.length.x = unit(.5, &quot;cm&quot;), axis.text = element_text(size = 14), legend.title = element_blank(), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(size = 12)) # save ggplots accordingly ggsave(fig_climate_elevWest, filename = &quot;figs/fig_westHim_elev100.png&quot;, height = 10, width = 14, device = png(), dpi = 300, units=&quot;in&quot;); dev.off() "],["processing-ebird-data.html", "Section 4 Processing eBird data 4.1 Load necessary libraries 4.2 Loading custom functions to process eBird data 4.3 Use the function written above to extract eBird data 4.4 Extract elevation at unique locations", " Section 4 Processing eBird data In this script, we will process the community science data across the Eastern and Western Himalayas. 4.1 Load necessary libraries Code library(tidyverse) library(sf) library(raster) library(parallel) 4.2 Loading custom functions to process eBird data Code # This function processes the eBird data as long as the path where the data is stored and list of countries are mentioned readcleanrawdata = function(rawpath) { require(lubridate) require(tidyverse) require(cowplot) preimp = c(&quot;COMMON.NAME&quot;,&quot;OBSERVATION.COUNT&quot;, &quot;LOCALITY.ID&quot;,&quot;LOCALITY.TYPE&quot;, &quot;STATE&quot;, &quot;COUNTRY&quot;, &quot;LATITUDE&quot;,&quot;LONGITUDE&quot;,&quot;OBSERVATION.DATE&quot;, &quot;TIME.OBSERVATIONS.STARTED&quot;,&quot;OBSERVER.ID&quot;, &quot;PROTOCOL.TYPE&quot;,&quot;DURATION.MINUTES&quot;,&quot;EFFORT.DISTANCE.KM&quot;, &quot;REVIEWED&quot;,&quot;NUMBER.OBSERVERS&quot;,&quot;ALL.SPECIES.REPORTED&quot;, &quot;GROUP.IDENTIFIER&quot;,&quot;SAMPLING.EVENT.IDENTIFIER&quot;,&quot;APPROVED&quot;, &quot;CATEGORY&quot;) nms = read.delim(rawpath, nrows = 1, sep = &quot;\\t&quot;, header = T, quote = &quot;&quot;, stringsAsFactors = F, na.strings = c(&quot;&quot;,&quot; &quot;,NA)) nms = names(nms) nms[!(nms %in% preimp)] = &quot;NULL&quot; nms[nms %in% preimp] = NA data = read.delim(rawpath, colClasses = nms, sep = &quot;\\t&quot;, header = T, quote = &quot;&quot;, stringsAsFactors = F, na.strings = c(&quot;&quot;,&quot; &quot;,NA)) ## choosing important variables imp = c(&quot;COMMON.NAME&quot;,&quot;OBSERVATION.COUNT&quot;, &quot;LOCALITY.ID&quot;,&quot;LOCALITY.TYPE&quot;, &quot;STATE&quot;, &quot;COUNTRY&quot;, &quot;LATITUDE&quot;,&quot;LONGITUDE&quot;,&quot;OBSERVATION.DATE&quot;, &quot;TIME.OBSERVATIONS.STARTED&quot;,&quot;OBSERVER.ID&quot;, &quot;PROTOCOL.TYPE&quot;,&quot;DURATION.MINUTES&quot;,&quot;EFFORT.DISTANCE.KM&quot;, &quot;SAMPLING.EVENT.IDENTIFIER&quot;, &quot;NUMBER.OBSERVERS&quot;,&quot;ALL.SPECIES.REPORTED&quot;,&quot;group.id&quot;, &quot;CATEGORY&quot;,&quot;no.sp&quot;) days = c(31,28,31,30,31,30,31,31,30,31,30,31) cdays = c(0,31,59,90,120,151,181,212,243,273,304,334) ## setup eBird data ## ## filter approved observations, species, slice by single group ID, remove repetitions ## remove repeats ## set date, add month, year and day columns using package LUBRIDATE ## filter distance travelled, duration birded and number of observers ## add number of species column (no.sp) data = data %&gt;% filter(REVIEWED == 0 | APPROVED == 1) %&gt;% mutate(group.id = ifelse(is.na(GROUP.IDENTIFIER), SAMPLING.EVENT.IDENTIFIER, GROUP.IDENTIFIER)) %&gt;% filter(ALL.SPECIES.REPORTED == 1) %&gt;% filter(PROTOCOL.TYPE == &quot;Stationary&quot;| PROTOCOL.TYPE == &quot;Traveling&quot;)%&gt;% filter(EFFORT.DISTANCE.KM&lt;=2.5|is.na(EFFORT.DISTANCE.KM))%&gt;% filter(DURATION.MINUTES &lt;= 120)%&gt;% filter(NUMBER.OBSERVERS &lt;= 10)%&gt;% mutate(Time = hms(TIME.OBSERVATIONS.STARTED)) %&gt;% filter(Time &gt; hms(&quot;4:00:00&quot;) &amp; Time &lt; hms(&quot;19:00:00&quot;))%&gt;% group_by(group.id,COMMON.NAME) %&gt;% slice(1) %&gt;% ungroup %&gt;% group_by(group.id) %&gt;% mutate(no.sp = n_distinct(COMMON.NAME))%&gt;% dplyr::select(imp) %&gt;% mutate(OBSERVATION.DATE = as.Date(OBSERVATION.DATE), month = month(OBSERVATION.DATE), year = year(OBSERVATION.DATE), day = day(OBSERVATION.DATE) + cdays[month], week = week(OBSERVATION.DATE), fort = ceiling(day/14)) %&gt;% filter(year &gt; 2010) ungroup return(data) } 4.3 Use the function written above to extract eBird data Code # please download the latest versions of eBird data from https://ebird.org/data/download and set the file path accordingly. Since these two datasets are extremely large, we have not uploaded the same to github. # In this study, the latest version of the data corresponds to August 31st 2022 # extract data for the following list of countries Bhutan &lt;- readcleanrawdata(&quot;ebd_BT_relAug-2022.txt&quot;) India &lt;- rbind(readcleanrawdata(&quot;ebd_IN-JK_relAug-2022.txt&quot;), readcleanrawdata(&quot;ebd_IN-LA_relAug-2022.txt&quot;), readcleanrawdata(&quot;ebd_IN-HP_relAug-2022.txt&quot;), readcleanrawdata(&quot;ebd_IN-AR_relAug-2022.txt&quot;), readcleanrawdata(&quot;ebd_IN-WB_relAug-2022.txt&quot;), readcleanrawdata(&quot;ebd_IN-UL_relAug-2022.txt&quot;), readcleanrawdata(&quot;ebd_IN-SK_relAug-2022.txt&quot;)) ## Removing non himalayan regions India&lt;-India %&gt;% filter(LATITUDE&gt;26,LONGITUDE&lt;100) dat &lt;-rbind(India,Bhutan) # Keep only unique locations used datll&lt;-dat%&gt;% filter(month %in% c(1,2,5,6,7,8,12)) %&gt;% distinct(LATITUDE,LONGITUDE, .keep_all = T)%&gt;%select(LOCALITY.ID,LATITUDE,LONGITUDE) write.csv(datll, &quot;results/unique_loc.csv&quot;, row.names = F) 4.4 Extract elevation at unique locations Code dat &lt;- st_as_sf(dat, coords = c(&quot;LONGITUDE&quot;,&quot;LATITUDE&quot;), crs=4326, remove = &quot;F&quot;) # Loading the elevation data elev &lt;- raster(&quot;data/elevation/alt&quot;) # extract elevation elevDat &lt;- raster::extract(elev,dat) # cbind elevation back to dataframe dat &lt;- cbind(dat,elevDat) # save Rdata file (uploaded to GitHub) save(dat, file = &quot;results/eBird_elev.RData&quot;) "],["resampling-analysis.html", "Section 5 Resampling analysis 5.1 Load .Rdata file containing elevation data across eBird sampling locations 5.2 Create resampled datasets", " Section 5 Resampling analysis Using the previously generated .Rdata file, we resample checklists for three levels of sampling effort at different elevational bands. 5.1 Load .Rdata file containing elevation data across eBird sampling locations Code load(&quot;results/eBird_elev.RData&quot;) dat &lt;- as.data.frame(dat) dat &lt;- dat [,-27] # removing unnecessary columns ## include only full species dat &lt;- dat %&gt;% filter(CATEGORY == &quot;species&quot; | CATEGORY == &quot;issf&quot;) # dividing eastern and western himalayas by the 83E longitude datWest &lt;- dat %&gt;% filter (LONGITUDE &lt; 83) datEast &lt;- dat %&gt;% filter (LONGITUDE &gt; 83) 5.2 Create resampled datasets Code # Creating resampled Dataset for the centre, lower, and upper limit of a species&#39; elevational distribution for 3 levels of sampling effort (number of checklists)- separately for east and west ## Eastern Himalayas ## Number of checklists in either season in each elevational band Checklists &lt;- datEast[!duplicated(datEast$group.id), ] Checklists$elev_level &lt;- cut(Checklists$elevation, breaks = c(-Inf, 500, 1000, 1500, 2000, 2500, 3000, Inf), labels = 1:7) Checklists.S &lt;- subset(Checklists, month %in% 5:8) # summer Checklists.W &lt;- subset(Checklists, month %in% c(1, 2, 12)) # winter summer &lt;- Checklists.S %&gt;% group_by(elev_level) %&gt;% summarise(summer = n_distinct(group.id)) winter &lt;- Checklists.W %&gt;% group_by(elev_level) %&gt;% summarise(winter = n_distinct(group.id)) ## if you want to output a table the number of checklists at each elevation band and season CL_ES &lt;- left_join(summer, winter, by = &quot;elev_level&quot;) levels(CL_ES$elev_level) &lt;- c(&quot;0-500&quot;,&quot;500-1000&quot;,&quot;1000-1500&quot;,&quot;1500-2000&quot;,&quot;2000-2500&quot;,&quot;2500-3000&quot;,&quot;&gt;3000&quot;) write.csv(CL_ES, &quot;results/ChecklistNo_SeasonElevation_East.csv&quot;, row.names = F) ## Sampling event IDs in each season and in each elevation band ID.S &lt;- lapply(1:7, function(x) Checklists.S$group.id[Checklists.S$elev_level == x]) ID.W &lt;- lapply(1:7, function(x) Checklists.W$group.id[Checklists.W$elev_level == x]) ## Get unique species list uniSpe &lt;- datEast %&gt;% filter(CATEGORY == &quot;species&quot; | CATEGORY == &quot;issf&quot;) uniSpe &lt;- unique(uniSpe[, &quot;COMMON.NAME&quot;]) %&gt;% data.frame() ## Get first second and third quartile of effort (number of checklists) across season and elevation efforts.S &lt;- summary(Checklists.S$elev_level) efforts.W &lt;- summary(Checklists.W$elev_level) qEffort &lt;- quantile(c(efforts.S, efforts.W), c(0.25,0.50,0.75)) # resample for the lower limit (5th percentile), center (median), and upper limit (95th percentile) for a species elevational distribution for (qt in c(0.05, 0.50, 0.95)) { #resample for the levels of sampling effort for (i in 1:3) { ## sample an equal number of checklists (3 levels of effort) from each elevation band set.seed(56789) sampleID.S &lt;- lapply(1:1000, function(y) {unlist(lapply(1:7, function(x) sample(ID.S[[x]], qEffort[i], replace=T)))}) set.seed(56789) sampleID.W &lt;- lapply(1:1000, function(y) {unlist(lapply(1:7, function(x) sample(ID.W[[x]], qEffort[i], replace=T)))}) # calculate the lower, median and upper elevation distribution for each bird species in the two seasons # This step takes awhile # We used parallel processing rs&lt;- mclapply(1:1000, function(y) { m &lt;- t(sapply(uniSpe[, 1], function(x){ occs.S &lt;- subset(datEast, COMMON.NAME ==x &amp; group.id %in% sampleID.S[[y]], select=&quot;elevation&quot;) occs.W &lt;- subset(datEast, COMMON.NAME==x &amp; group.id %in% sampleID.W[[y]], select=&quot;elevation&quot;) B.n &lt;- nrow(occs.S) W.n &lt;- nrow(occs.W) B.elev &lt;- if (B.n &gt; 0) quantile(occs.S$elevation, qt) else NA W.elev &lt;- if (W.n &gt; 0) quantile(occs.W$elevation, qt) else NA return(c(B.elev = B.elev, B.n = B.n, W.elev = W.elev, W.n = W.n)) })) }, mc.cores = 7) save(rs,file = paste0(&quot;results/eBird_woNepal_resampled_east_&quot;, sprintf(&quot;%02d&quot;, qt*100),&quot;.q&quot;, i ,&quot;.RData&quot;)) } } ## Repeat the above process for Western Himalayas Checklists &lt;- datWest[!duplicated(datWest$group.id), ] Checklists$elev_level &lt;- cut(Checklists$elevation, breaks = c(-Inf, 500, 1000, 1500, 2000, 2500, 3000, Inf), labels = 1:7) Checklists.S &lt;- subset(Checklists, month %in% 5:8) Checklists.W &lt;- subset(Checklists, month %in% c(1, 2, 12)) summer &lt;- Checklists.S %&gt;% group_by(elev_level) %&gt;% summarise(summer = n_distinct(group.id)) winter &lt;- Checklists.W %&gt;% group_by(elev_level) %&gt;% summarise(winter = n_distinct(group.id)) CL_ES &lt;- left_join(summer,winter, by = &quot;elev_level&quot;) levels(CL_ES$elev_level) &lt;-c(&quot;0-500&quot;,&quot;500-1000&quot;,&quot;1000-1500&quot;,&quot;1500-2000&quot;,&quot;2000-2500&quot;,&quot;2500-3000&quot;,&quot;&gt;3000&quot;) write.csv(CL_ES, &quot;results/ChecklistNo_SeasonElevation_West.csv&quot;, row.names = F ) ID.S &lt;- lapply(1:7, function(x) Checklists.S$group.id[Checklists.S$elev_level == x]) ID.W &lt;- lapply(1:7, function(x) Checklists.W$group.id[Checklists.W$elev_level == x]) uniSpe &lt;- datWest %&gt;% filter(CATEGORY == &quot;species&quot; | CATEGORY == &quot;issf&quot;) uniSpe &lt;- unique(uniSpe[, &quot;COMMON.NAME&quot;]) %&gt;% data.frame() efforts.S &lt;- summary(Checklists.S$elev_level) efforts.W &lt;- summary(Checklists.W$elev_level) qEffort &lt;- quantile(c(efforts.S, efforts.W), c(0.25,0.50,0.75)) for (qt in c(0.05, 0.50, 0.95)) { for (i in 1:3) { set.seed(56789) sampleID.S &lt;- lapply(1:1000, function(y) {unlist(lapply(1:7, function(x) sample(ID.S[[x]], qEffort[i], replace=T)))}) set.seed(56789) sampleID.W &lt;- lapply(1:1000, function(y) {unlist(lapply(1:7, function(x) sample(ID.W[[x]], qEffort[i], replace=T)))}) rs&lt;- mclapply(1:1000, function(y) { m &lt;- t(sapply(uniSpe[, 1], function(x){ occs.S &lt;- subset(datWest, COMMON.NAME ==x &amp; group.id %in% sampleID.S[[y]], select=&quot;elevation&quot;) occs.W &lt;- subset(datWest, COMMON.NAME==x &amp; group.id %in% sampleID.W[[y]], select=&quot;elevation&quot;) B.n &lt;- nrow(occs.S) W.n &lt;- nrow(occs.W) B.elev &lt;- if (B.n &gt; 0) quantile(occs.S$elevation, qt) else NA W.elev &lt;- if (W.n &gt; 0) quantile(occs.W$elevation, qt) else NA return(c(B.elev = B.elev, B.n = B.n, W.elev = W.elev, W.n = W.n)) })) }, mc.cores = 7 ) save(rs,file = paste0(&quot;results/eBird_woNepal_resampled_west_&quot;, sprintf(&quot;%02d&quot;, qt*100),&quot;.q&quot;, i,&quot;.RData&quot;)) } } "],["extent-of-elevational-migration.html", "Section 6 Extent of elevational migration 6.1 Load necessary libraries 6.2 Load previously generated .Rdata files for analyses 6.3 Eastern Himalayas 6.4 Western Himalayas", " Section 6 Extent of elevational migration In this script, we calculate the median migration extent, breeding and wintering elevation for all combinations of elevational limit and sampling effort separately for the eastern and western Himalayas, modified from Tsai et al 2020) 6.1 Load necessary libraries Code library(ape) library(geiger) library(phytools) library(coxme) library(evobiR) library(nlme) library(tidyverse) library(phangorn) 6.2 Load previously generated .Rdata files for analyses Code load(&quot;results/eBird_elev.RData&quot;) dat.1&lt;-as.data.frame(dat.1) dat.1&lt;-dat.1[,-27] #### Keep only full species data dat.1&lt;-dat.1 %&gt;% filter(CATEGORY == &quot;species&quot; | CATEGORY == &quot;issf&quot;) ##Separating eastern and western himalayas dat1W&lt;-dat.1 %&gt;% filter (LONGITUDE &lt; 83) dat1E&lt;-dat.1 %&gt;% filter (LONGITUDE &gt; 83) rm(dat.1) 6.3 Eastern Himalayas Code #### Creating a list of unique species uniSpe_east &lt;- dat1E %&gt;% filter(CATEGORY == &quot;species&quot; | CATEGORY == &quot;issf&quot;) uniSpe_east &lt;- unique(uniSpe_east[, &quot;COMMON.NAME&quot;]) %&gt;% data.frame() colnames(uniSpe_east) &lt;- &quot;Species&quot; for (minSample in c(30, 60)) { for (ds in c(&quot;05.q1&quot;, &quot;05.q2&quot;, &quot;05.q3&quot;, &quot;95.q1&quot;, &quot;95.q2&quot;, &quot;95.q3&quot;, &quot;50.q1&quot;, &quot;50.q2&quot;, &quot;50.q3&quot;)) { load(paste(&quot;results/eBird_woNepal_resampled_east_&quot;, ds, &quot;.Rdata&quot;, sep = &quot;&quot;)) B.elev &lt;- sapply(1:1000, function(x) rs[[x]][, 1]) B.n &lt;- sapply(1:1000, function(x) rs[[x]][, 2]) W.elev &lt;- sapply(1:1000, function(x) rs[[x]][, 3]) W.n &lt;- sapply(1:1000, function(x) rs[[x]][, 4]) diff &lt;- B.elev - W.elev dimnames(diff)&lt;-list(uniSpe_east[,1],1:1000) # For each species exclude trials where it has been detected less than 30/60 times in winter or summer if (minSample == 30) diff[W.n &lt; 30 | B.n &lt; 30] &lt;- NA if (minSample == 60) diff[W.n &lt; 60 | B.n &lt; 60] &lt;- NA diff.sel &lt;- diff[apply(diff, 1, FUN = function(x) sum(is.na(x))) &lt; 1000, ] # Calculate medians of the percentiles over 1000 sets of resampled sampling events. Along with confidence intervals med.CI &lt;- apply(diff.sel, 1, FUN = function(x) quantile((x), c(0.025, .5, .975), na.rm = TRUE)) med.CI &lt;- t(med.CI) med.CI &lt;- as.data.frame(med.CI) med.CI$Group &lt;- with(med.CI, ifelse(`2.5%` &gt; 0 &amp; `97.5%` &gt; 0, 2, ifelse(`2.5%` &lt; 0 &amp; `97.5%` &lt; 0, 1, 0))) med.CI$LCI_diff &lt;- as.numeric(med.CI$`2.5%`) med.CI$Median_diff &lt;- as.numeric(med.CI$`50%`) med.CI$UCI_diff &lt;- as.numeric(med.CI$`97.5%`) med.CI$Species &lt;- rownames(med.CI) med.CI&lt;-med.CI[,-c(1,2,3)] ## adding median breeding elevations to the same table dimnames(B.elev)&lt;-list(uniSpe_east[,1],1:1000) if (minSample == 30) B.elev[W.n &lt; 30 | B.n &lt; 30] &lt;- NA if (minSample == 60) B.elev[W.n &lt; 60 | B.n &lt; 60] &lt;- NA B.sel &lt;- B.elev[apply(B.elev, 1, FUN = function(x) sum(is.na(x))) &lt; 1000, ] med.S.CI &lt;- apply(B.sel, 1, FUN = function(x) quantile((x), c(0.025, .5, .975), na.rm = TRUE)) med.S.CI &lt;- t(med.S.CI) med.S.CI &lt;- as.data.frame(med.S.CI) med.S.CI$LCI_S &lt;- as.numeric(med.S.CI$`2.5%`) med.S.CI$Median_S &lt;- as.numeric(med.S.CI$`50%`) med.S.CI$UCI_S &lt;- as.numeric(med.S.CI$`97.5%`) med.S.CI$Species &lt;- rownames(med.S.CI) med.S.CI&lt;-med.S.CI[,-c(1,2,3)] ## adding median winter elevations to the same table dimnames(W.elev)&lt;-list(uniSpe_east[,1],1:1000) if (minSample == 30) W.elev[W.n &lt; 30 | B.n &lt; 30] &lt;- NA if (minSample == 60) W.elev[W.n &lt; 60 | B.n &lt; 60] &lt;- NA W.sel &lt;- W.elev[apply(W.elev, 1, FUN = function(x) sum(is.na(x))) &lt; 1000, ] med.W.CI &lt;- apply(W.sel, 1, FUN = function(x) quantile((x), c(0.025, .5, .975), na.rm = TRUE)) med.W.CI &lt;- t(med.W.CI) med.W.CI &lt;- as.data.frame(med.W.CI) med.W.CI$LCI_W &lt;- as.numeric(med.W.CI$`2.5%`) med.W.CI$Median_W &lt;- as.numeric(med.W.CI$`50%`) med.W.CI$UCI_W &lt;- as.numeric(med.W.CI$`97.5%`) med.W.CI$Species &lt;- rownames(med.W.CI) med.W.CI&lt;-med.W.CI[,-c(1,2,3)] ts&lt;-left_join(med.CI,med.S.CI, by = &quot;Species&quot;) %&gt;% left_join(., med.W.CI, by = &quot;Species&quot;, ) ts &lt;- ts[c(&quot;Species&quot;, &quot;Group&quot;, &quot;Median_S&quot;, &quot;LCI_S&quot;, &quot;UCI_S&quot;,&quot;Median_W&quot;,&quot;LCI_W&quot;,&quot;UCI_W&quot;,&quot;Median_diff&quot;,&quot;LCI_diff&quot;, &quot;UCI_diff&quot;)] write.csv(ts, file = paste(&quot;results/final_birdlist_east_&quot;,ds,&quot;.sam&quot;,minSample, &quot;.csv&quot;, sep = &quot;&quot;), row.names = F) } } 6.4 Western Himalayas Code uniSpe_west &lt;- dat1W %&gt;% filter(CATEGORY == &quot;species&quot; | CATEGORY == &quot;issf&quot;) uniSpe_west &lt;- unique(uniSpe_west[, &quot;COMMON.NAME&quot;]) %&gt;% data.frame() colnames(uniSpe_west) &lt;- &quot;Species&quot; for (minSample in c(30, 60)) { for (ds in c(&quot;05.q1&quot;, &quot;05.q2&quot;, &quot;05.q3&quot;, &quot;95.q1&quot;, &quot;95.q2&quot;, &quot;95.q3&quot;, &quot;50.q1&quot;, &quot;50.q2&quot;, &quot;50.q3&quot;)) { load(paste(&quot;results/eBird_woNepal_resampled_west_&quot;, ds, &quot;.Rdata&quot;, sep = &quot;&quot;)) B.elev &lt;- sapply(1:1000, function(x) rs[[x]][, 1]) B.n &lt;- sapply(1:1000, function(x) rs[[x]][, 2]) W.elev &lt;- sapply(1:1000, function(x) rs[[x]][, 3]) W.n &lt;- sapply(1:1000, function(x) rs[[x]][, 4]) diff &lt;- B.elev - W.elev dimnames(diff)&lt;-list(uniSpe_west[,1],1:1000) # For each species exclude trials where it has been detected less than 30/60 times in winter or summer if (minSample == 30) diff[W.n &lt; 30 | B.n &lt; 30] &lt;- NA if (minSample == 60) diff[W.n &lt; 60 | B.n &lt; 60] &lt;- NA diff.sel &lt;- diff[apply(diff, 1, FUN = function(x) sum(is.na(x))) &lt; 1000, ] # Calculate medians of the percentiles over 1000 sets of resampled sampling events. Along with confidence intervals med.CI &lt;- apply(diff.sel, 1, FUN = function(x) quantile((x), c(0.025, .5, .975), na.rm = TRUE)) med.CI &lt;- t(med.CI) med.CI &lt;- as.data.frame(med.CI) med.CI$Group &lt;- with(med.CI, ifelse(`2.5%` &gt; 0 &amp; `97.5%` &gt; 0, 2, ifelse(`2.5%` &lt; 0 &amp; `97.5%` &lt; 0, 1, 0))) med.CI$LCI_diff &lt;- as.numeric(med.CI$`2.5%`) med.CI$Median_diff &lt;- as.numeric(med.CI$`50%`) med.CI$UCI_diff &lt;- as.numeric(med.CI$`97.5%`) med.CI$Species &lt;- rownames(med.CI) med.CI&lt;-med.CI[,-c(1,2,3)] ## adding median breeding elevations to the same table dimnames(B.elev)&lt;-list(uniSpe_west[,1],1:1000) if (minSample == 30) B.elev[W.n &lt; 30 | B.n &lt; 30] &lt;- NA if (minSample == 60) B.elev[W.n &lt; 60 | B.n &lt; 60] &lt;- NA B.sel &lt;- B.elev[apply(B.elev, 1, FUN = function(x) sum(is.na(x))) &lt; 1000, ] med.S.CI &lt;- apply(B.sel, 1, FUN = function(x) quantile((x), c(0.025, .5, .975), na.rm = TRUE)) med.S.CI &lt;- t(med.S.CI) med.S.CI &lt;- as.data.frame(med.S.CI) med.S.CI$LCI_S &lt;- as.numeric(med.S.CI$`2.5%`) med.S.CI$Median_S &lt;- as.numeric(med.S.CI$`50%`) med.S.CI$UCI_S &lt;- as.numeric(med.S.CI$`97.5%`) med.S.CI$Species &lt;- rownames(med.S.CI) med.S.CI&lt;-med.S.CI[,-c(1,2,3)] ## adding median winter elevations to the same table dimnames(W.elev)&lt;-list(uniSpe_west[,1],1:1000) if (minSample == 30) W.elev[W.n &lt; 30 | B.n &lt; 30] &lt;- NA if (minSample == 60) W.elev[W.n &lt; 60 | B.n &lt; 60] &lt;- NA W.sel &lt;- W.elev[apply(W.elev, 1, FUN = function(x) sum(is.na(x))) &lt; 1000, ] med.W.CI &lt;- apply(W.sel, 1, FUN = function(x) quantile((x), c(0.025, .5, .975), na.rm = TRUE)) med.W.CI &lt;- t(med.W.CI) med.W.CI &lt;- as.data.frame(med.W.CI) med.W.CI$LCI_W &lt;- as.numeric(med.W.CI$`2.5%`) med.W.CI$Median_W &lt;- as.numeric(med.W.CI$`50%`) med.W.CI$UCI_W &lt;- as.numeric(med.W.CI$`97.5%`) med.W.CI$Species &lt;- rownames(med.W.CI) med.W.CI&lt;-med.W.CI[,-c(1,2,3)] ts&lt;-left_join(med.CI,med.S.CI, by = &quot;Species&quot;) %&gt;% left_join(., med.W.CI, by = &quot;Species&quot;, ) ts &lt;- ts[c(&quot;Species&quot;, &quot;Group&quot;, &quot;Median_S&quot;, &quot;LCI_S&quot;, &quot;UCI_S&quot;,&quot;Median_W&quot;,&quot;LCI_W&quot;,&quot;UCI_W&quot;,&quot;Median_diff&quot;,&quot;LCI_diff&quot;, &quot;UCI_diff&quot;)] write.csv(ts, file = paste(&quot;results/final_birdlist_west&quot;,ds,&quot;.sam&quot;,minSample, &quot;.csv&quot;, sep = &quot;&quot;), row.names = F) } } "],["phylogenetic-generalized-least-squares-regressions.html", "Section 7 Phylogenetic generalized least squares regressions 7.1 Remove non-Himalayan species 7.2 PGLS models for the Eastern Himalayas 7.3 PGLS models for the Western Himalayas 7.4 Phylogenetic paired t-test for species common to the east and west", " Section 7 Phylogenetic generalized least squares regressions Here, we used phylogenetic generalized least squares regression to test if thermal regime, dispersal ability, and diet significantly drive Himalayan bird elevational shift. 7.1 Remove non-Himalayan species Code # first we remove species that are non-Himalayan rem &lt;-read.csv(&quot;data/SpeciesList_nonhimalayan.csv.csv&quot;) 7.2 PGLS models for the Eastern Himalayas Code for (ds in c(&quot;05.q1.sam30&quot;, &quot;05.q1.sam60&quot;, &quot;05.q2.sam30&quot;,&quot;05.q2.sam60&quot;, &quot;05.q3.sam30&quot;,&quot;05.q3.sam60&quot;, &quot;95.q1.sam30&quot;,&quot;95.q1.sam60&quot;, &quot;95.q2.sam30&quot;,&quot;95.q2.sam60&quot; ,&quot;95.q3.sam30&quot;,&quot;95.q3.sam60&quot; ,&quot;50.q1.sam30&quot;,&quot;50.q1.sam60&quot; ,&quot;50.q2.sam30&quot;,&quot;50.q2.sam60&quot; ,&quot;50.q3.sam30&quot;, &quot;50.q3.sam60&quot;)) { x&lt;-read.csv(paste(&quot;results/final_birdlist_east_&quot;, ds, &quot;.csv&quot;, sep = &quot;&quot;)) x&lt;-x[!(x$Species %in% rem$Species),] write.csv(x,file = paste(&quot;results/birdlist_east&quot;,ds,&quot;.csv&quot;, sep = &quot;&quot;), row.names = F) # birdlist without non-himalayan species } # load necessary data for running statistical models east_100 &lt;-read.csv(&quot;results/eastHim_100.csv&quot;) #temperature data east_100_S &lt;- east_100 %&gt;% select(-contains(&quot;Jan&quot;)) %&gt;% rename(elev_roundS = elev_round) #winter east_100_W &lt;- east_100 %&gt;% select(-contains(&quot;June&quot;))%&gt;% rename(elev_roundW = elev_round) #summer jetzname &lt;-read.csv(&quot;data/for_PGLS_list3.csv&quot;) #taxonomy used by birdtree.org sheard_trait &lt;- read.csv(&quot;data/2020-sheard et al-species-trait-dat - speciesdata.csv&quot;) %&gt;% #trait data set select(HWI,Diet,Tree.name) tree &lt;-read.nexus(&quot;data/birdtree.nex&quot;) #nex file from birdtree.org (hackett all species) tree &lt;- mcc(tree) ## maximum clade credibility tree # create a single dataframe of summaries of all model combinations compile_eastmodels&lt;-setNames(data.frame(matrix(ncol = 6, nrow = 0)), c(&quot;Variable&quot;, &quot;Value&quot;, &quot;Std.Error&quot;, &quot;t-value&quot;, &quot;p-value&quot;, &quot;ds&quot;)) for (ds in c(&quot;05.q1.sam30&quot;, &quot;05.q1.sam60&quot;, &quot;05.q2.sam30&quot;,&quot;05.q2.sam60&quot;, &quot;05.q3.sam30&quot;,&quot;05.q3.sam60&quot;, &quot;95.q1.sam30&quot;,&quot;95.q1.sam60&quot;, &quot;95.q2.sam30&quot;,&quot;95.q2.sam60&quot; ,&quot;95.q3.sam30&quot;,&quot;95.q3.sam60&quot; ,&quot;50.q1.sam30&quot;,&quot;50.q1.sam60&quot; ,&quot;50.q2.sam30&quot;,&quot;50.q2.sam60&quot; ,&quot;50.q3.sam30&quot;, &quot;50.q3.sam60&quot;)) { x&lt;-read.csv(paste(&quot;results/birdlist_east&quot;, ds, &quot;.csv&quot;, sep = &quot;&quot;)) x$elev_roundS&lt;-round(x$Median_S,-2) x$elev_roundW&lt;-round(x$Median_W,-2) x&lt;-left_join(x,east_100_S, by = &quot;elev_roundS&quot;) #aligning temperature data to elevational range data x&lt;-left_join(x,east_100_W, by = &quot;elev_roundW&quot;) x$ThermalRegime&lt;-x$maxTemp_June_mean-x$minTemp_Jan_mean #calculating species thermal regimes x&lt;-left_join(x,jetzname, by = &quot;Species&quot;) #adding jetz taxonomy x&lt;-left_join(x, sheard_trait, by = &quot;Tree.name&quot;)%&gt;% mutate(Diet = fct_recode(Diet, &quot;omnivore&quot; = &quot;nectar&quot;, &quot;omnivore&quot; = &quot;scav&quot;, &quot;seeds&quot;=&quot;plants&quot;)) #adding trait info pruned.tree&lt;-drop.tip(tree1,tree1$tip.label[-match(x$Tree.name, tree1$tip.label)]) #setting up the PGLS model glsdata&lt;-x[match(pruned.tree$tip.label, x$Tree.name),] #setting up the PGLS model pgls2&lt;-gls(Median_diff~ThermalRegime+HWI+relevel(Diet, ref = &quot;invertebrates&quot;),correlation=corPagel(1,pruned.tree),data=glsdata) #setting up the PGLS model k&lt;-summary(pgls2) outputk&lt;-as.data.frame(k$tTable) outputk$dataset&lt;-ds outputk$Variable&lt;-rownames(outputk) compile_eastmodels&lt;-rbind(compile_eastmodels,outputk) write.csv(glsdata, file = paste(&quot;results/east_ordered&quot;,ds,&quot;.csv&quot;, sep = &quot;&quot;), row.names = F) # species ordered by taxonomy write.csv(compile_eastmodels, &quot;results/compile_eastmodels.csv&quot;, row.names = F) } ## calculating adjusted p values using the Benjamini Hochberg correction for false discovery rates east_p&lt;-read.csv(&quot;results/compile_eastmodels.csv&quot;) east_p$adjP&lt;-p.adjust(east_p$p.value, method = &quot;fdr&quot;) write.csv(east_p, &quot;results/compile_eastmodels_padj.csv&quot;) 7.3 PGLS models for the Western Himalayas Code for (ds in c(&quot;05.q1.sam30&quot;, &quot;05.q1.sam60&quot;, &quot;05.q2.sam30&quot;,&quot;05.q2.sam60&quot;, &quot;05.q3.sam30&quot;,&quot;05.q3.sam60&quot;, &quot;95.q1.sam30&quot;,&quot;95.q1.sam60&quot;, &quot;95.q2.sam30&quot;,&quot;95.q2.sam60&quot; ,&quot;95.q3.sam30&quot;,&quot;95.q3.sam60&quot; ,&quot;50.q1.sam30&quot;,&quot;50.q1.sam60&quot; ,&quot;50.q2.sam30&quot;,&quot;50.q2.sam60&quot; ,&quot;50.q3.sam30&quot;, &quot;50.q3.sam60&quot;)) { x&lt;-read.csv(paste(&quot;results/final_birdlist_west&quot;, ds, &quot;.csv&quot;, sep = &quot;&quot;)) x&lt;-x[!(x$Species %in% rem$Species),] write.csv(x,file = paste(&quot;results/birdlist_west&quot;,ds,&quot;.csv&quot;, sep = &quot;&quot;), row.names = F) ## final list after removing non himalayan species } west_100&lt;-read.csv(&quot;results/westHim_100.csv&quot;) #temperature data west_100_S&lt;-west_100 %&gt;% select(-contains(&quot;Jan&quot;)) %&gt;% rename(elev_roundS = elev_round) #winter west_100_W&lt;-west_100 %&gt;% select(-contains(&quot;June&quot;))%&gt;% rename(elev_roundW = elev_round) #summer # create a single dataframe of summaries of all model combinations compile_westmodels&lt;-setNames(data.frame(matrix(ncol = 5, nrow = 0)), c(&quot;Value&quot;, &quot;Std.Error&quot;, &quot;t-value&quot;, &quot;p-value&quot;, &quot;ds&quot;)) for (ds in c(&quot;05.q1.sam30&quot;, &quot;05.q1.sam60&quot;, &quot;05.q2.sam30&quot;,&quot;05.q2.sam60&quot;, &quot;05.q3.sam30&quot;,&quot;05.q3.sam60&quot;, &quot;95.q1.sam30&quot;,&quot;95.q1.sam60&quot;, &quot;95.q2.sam30&quot;,&quot;95.q2.sam60&quot; ,&quot;95.q3.sam30&quot;,&quot;95.q3.sam60&quot; ,&quot;50.q1.sam30&quot;,&quot;50.q1.sam60&quot; ,&quot;50.q2.sam30&quot;,&quot;50.q2.sam60&quot; ,&quot;50.q3.sam30&quot;, &quot;50.q3.sam60&quot;)) { x&lt;-read.csv(paste(&quot;results/birdlist_west&quot;, ds, &quot;.csv&quot;, sep = &quot;&quot;)) x$elev_roundS&lt;-round(x$Median_S,-2) x$elev_roundW&lt;-round(x$Median_W,-2) x&lt;-left_join(x,west_100_S, by = &quot;elev_roundS&quot;) #aligning temperature data to elevational range data x&lt;-left_join(x,west_100_W, by = &quot;elev_roundW&quot;) x$ThermalRegime&lt;-x$maxTemp_June_mean-x$minTemp_Jan_mean #calculating species thermal regimes x&lt;-left_join(x,jetzname, by = &quot;Species&quot;) #adding jetz taxonomy x&lt;-left_join(x, sheard_trait, by = &quot;Tree.name&quot;)%&gt;% mutate(Diet = fct_recode(Diet, &quot;omnivore&quot; = &quot;nectar&quot;, &quot;omnivore&quot; = &quot;scav&quot;, &quot;seeds&quot;=&quot;plants&quot;)) pruned.tree&lt;-drop.tip(tree1,tree1$tip.label[-match(x$Tree.name, tree1$tip.label)]) #setting up the PGLS model glsdata&lt;-x[match(pruned.tree$tip.label, x$Tree.name),] #setting up the PGLS model pgls2&lt;-gls(Median_diff~ThermalRegime+HWI+relevel(Diet, ref = &quot;invertebrates&quot;),correlation=corPagel(1,pruned.tree),data=glsdata) #setting up the PGLS model k&lt;-summary(pgls2) outputk&lt;-as.data.frame(k$tTable) outputk$dataset&lt;-ds outputk$Variable&lt;-rownames(outputk) compile_westmodels&lt;-rbind(compile_westmodels,outputk) write.csv(glsdata, file = paste(&quot;results/west_ordered&quot;,ds,&quot;.csv&quot;, sep = &quot;&quot;), row.names = F) # species ordered by taxonomy write.csv(compile_westmodels, &quot;results/compile_westmodels.csv&quot;, row.names = F) } ## calculating adjusted p values using the Benjamini Hochberg correction for false discovery rates west_p&lt;-read.csv(&quot;results/compile_westmodels.csv&quot;) west_p$adjP&lt;-p.adjust(west_p$p.value, method = &quot;fdr&quot;) write.csv(east_p, &quot;results/compile_westmodels_padj.csv&quot;) 7.4 Phylogenetic paired t-test for species common to the east and west Code diff_ttest&lt;-setNames(data.frame(matrix(ncol = 3, nrow = 0)), c(&quot;t&quot;, &quot;df&quot;, &quot;pvalue&quot;)) # difference in extent of elevation shift niche_ttest&lt;-setNames(data.frame(matrix(ncol = 3, nrow = 0)), c(&quot;t&quot;, &quot;df&quot;, &quot;pvalue&quot;)) #difference in thermal regimes for (ds in c(&quot;05.q1.sam30&quot;, &quot;05.q1.sam60&quot;, &quot;05.q2.sam30&quot;,&quot;05.q2.sam60&quot;, &quot;05.q3.sam30&quot;,&quot;05.q3.sam60&quot;, &quot;95.q1.sam30&quot;,&quot;95.q1.sam60&quot;, &quot;95.q2.sam30&quot;,&quot;95.q2.sam60&quot; ,&quot;95.q3.sam30&quot;,&quot;95.q3.sam60&quot; ,&quot;50.q1.sam30&quot;,&quot;50.q1.sam60&quot; ,&quot;50.q2.sam30&quot;,&quot;50.q2.sam60&quot; ,&quot;50.q3.sam30&quot;, &quot;50.q3.sam60&quot;)) { west&lt;-read.csv(paste(&quot;results/west_ordered&quot;,ds,&quot;.csv&quot;, sep = &quot;&quot;)) east&lt;-read.csv(paste(&quot;results/east_ordered&quot;,ds,&quot;.csv&quot;, sep = &quot;&quot;)) commonW&lt;-west[(west$Species %in% east$Species),] # subset of species in the west that are found in the east commonE&lt;-east[(east$Species %in% west$Species),] # subset of species in the east that are found in the west common&lt;-data.frame(Median_diffW = commonW$Median_diff,Median_diffE = commonE$Median_diff, ThermalRegimeW = commonW$ThermalRegime,ThermalRegimeE = commonE$ThermalRegime ,Tree.name = commonE$Tree.name) # combined dataframe of common species with their extent of shift and thermal regimes in both the eastern and western himalayas # setting up and running the phylogenetic t-tests and outputting results on csvs pruned.tree&lt;-drop.tip(tree1,tree1$tip.label[-match(common$Tree.name, tree1$tip.label)]) comdata&lt;-common[match(pruned.tree$tip.label, common$Tree.name),] row.names(comdata)&lt;-comdata$Tree.name a&lt;-phyl.pairedttest(pruned.tree,comdata[,c(1,2)]) b&lt;-phyl.pairedttest(pruned.tree,comdata[,c(3,4)]) a_diff_ttest&lt;-data.frame(t = a$t, pvalue = a$P, df = a$df, dataset = ds) b_niche_ttest&lt;-data.frame(t = b$t, pvalue = b$P, df = b$df, dataset = ds) diff_ttest&lt;-rbind(diff_ttest,a_diff_ttest) niche_ttest&lt;-rbind(niche_ttest,b_niche_ttest) write.csv(diff_ttest, &quot;results/diff_ttest.csv&quot;, row.names = F) # difference in extent of elevation shift write.csv(niche_ttest, &quot;results/niche_ttest.csv&quot;, row.names = F) #difference in thermal regimes } "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
