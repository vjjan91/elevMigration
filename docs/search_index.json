[["index.html", "Source code for High elevation Himalayan birds shift elevations to track thermal regimes across seasons Section 1 Introduction 1.1 Data processing 1.2 Attribution 1.3 Data access 1.4 The data used in this work is archived on Zenodo.", " Source code for High elevation Himalayan birds shift elevations to track thermal regimes across seasons Sahas Barve Tarun Menon Vijay Ramesh 2022-05-13 Section 1 Introduction This is the readable version containing analysis that models the extent of elevational migration in Himalayan birds using community science data (eBird). 1.1 Data processing The data processing for this project is described in the following sections. Navigate through them using the links in the sidebar. 1.2 Attribution Please contact the following in case of interest in the project. Vijay Ramesh (repo maintainer) PhD student, Columbia University 1.3 Data access 1.4 The data used in this work is archived on Zenodo. Spatially thinned checklist locations of bird observations across the Himalayas. Map showing sampling points included in our dataset for elevational migration in Himalayan birds. For ease of visualization, we spatially thinned localities to show only 710 unique locations, by ensuring a minimum distance of fifteen kilometres between each checklist locality. This process was carried using the thin function from the spThin package within the R programming environment (Aiello-Lammens et al., 2019; R Core Team, 2021). This map was created in ArcGIS Pro with a shaded relief from a high-resolution digital elevation model (SRTM). "],["spatial-thinning-of-occurrence-data.html", "Section 2 Spatial thinning of occurrence data 2.1 Load necessary libraries 2.2 Load data", " Section 2 Spatial thinning of occurrence data In this script, we spatially thin occurrence data for creating Figure 1. 2.1 Load necessary libraries # load libraries library(sf) library(dplyr) library(spThin) 2.2 Load data # load sheet of all localties loc &lt;- read.csv(&quot;data/localities-for-map.csv&quot;) head(loc) loc_unique &lt;- loc %&gt;% distinct() loc_unique$species &lt;- &quot;species&quot; # carry out spatial thinning with a minimum distance of 15km between records thinned &lt;- thin(loc_unique,lat.col=&quot;LATITUDE&quot;,long.col = &quot;LONGITUDE&quot;, spec.col = &quot;species&quot;,thin.par = 15,reps=1, write.files=T, out.dir=&quot;data/&quot;, out.base=&quot;localities-thinned&quot;) # reload spatially thinned file thin_file &lt;- read.csv(&quot;results/localities-for-map-thinned.csv&quot;) thin_file &lt;- thin_file %&gt;% mutate(region = case_when(LONGITUDE &lt; 83 ~ &quot;west&quot;, LONGITUDE &gt; 83 ~ &quot;east&quot;)) shp &lt;- st_as_sf(thin_file, coords = c(&quot;LONGITUDE&quot;,&quot;LATITUDE&quot;), crs=4326) str(shp) st_write(shp[shp$region==&quot;east&quot;,],&quot;data/shapefiles/localities-east.shp&quot;, driver=&quot;ESRI Shapefile&quot;) st_write(shp[shp$region==&quot;west&quot;,],&quot;data/shapefiles/localities-west.shp&quot;, driver=&quot;ESRI Shapefile&quot;) "],["temperature-elevation-associations.html", "Section 3 Temperature-elevation associations 3.1 Load necessary libraries 3.2 Load shapefiles 3.3 Prepare elevation rasters 3.4 Prepare climate rasters 3.5 Extracting data across elevational bands 3.6 Plot temperature as a function of elevation", " Section 3 Temperature-elevation associations In this script, we calculate temperatures across elevational bands in the Eastern and Western Himalayas. 3.1 Load necessary libraries library(raster) library(stringi) library(glue) library(gdalUtils) library(purrr) library(dplyr) library(tidyr) library(scales) library(ggplot2) library(ggthemes) library(sf) library(mapview) library(rgeos) # get ci func ci &lt;- function(x){qnorm(0.975)*sd(x, na.rm = T)/sqrt(length(x))} # prep mode function to aggregate funcMode &lt;- function(x, na.rm = T) { ux &lt;- unique(x) ux[which.max(tabulate(match(x, ux)))] } # a basic test assertthat::assert_that(funcMode(c(2,2,2,2,3,3,3,4)) == as.character(2), msg = &quot;problem in the mode function&quot;) # works 3.2 Load shapefiles # Load shapefiles # Please note the below shapefile consists of multiple polygons that combines the east and west himalayas &lt;- st_read(&quot;data/shapefiles/shapefile_himalaya.shp&quot;) mapview(himalayas) # Need to merge a few polygons to essentially split himalayas into the west and the east # This will have to be done at 83E for Nepal # Merge polygons for western himalayas west_toMerge &lt;- himalayas[c(3,4,7,8,9,10),] westHim &lt;- st_crop(west_toMerge,xmin=68.03321,ymin=23.69771,xmax=83,ymax=37.07761) # Merge polygons for eastern himalayas east_toMerge &lt;- himalayas[c(1,2,5,6,10),] east_toMerge &lt;- st_buffer(east_toMerge, dist=0) eastHim &lt;- st_crop(east_toMerge,xmin=83,ymin=25.96462,xmax=97.4115,ymax=30.44728) 3.3 Prepare elevation rasters # load elevation and crop to hills size, then mask by hills # Please note that this file is large and is not uploaded to GitHub and can be downloaded from SRTM (Farr et al. 2007) alt &lt;- raster(&quot;data/elevation/alt&quot;) alt.east &lt;- crop(alt, as(eastHim, &quot;Spatial&quot;)) alt.west &lt;- crop(alt, as(westHim, &quot;Spatial&quot;)) rm(alt); gc() # get slope and aspect slopeEast &lt;- terrain(x = alt.east, opt = c(&quot;slope&quot;, &quot;aspect&quot;)) slopeWest &lt;- terrain(x = alt.west, opt = c(&quot;slope&quot;, &quot;aspect&quot;)) # stack rasters elevEast &lt;- raster::stack(alt.east, slopeEast) elevWest &lt;- raster::stack(alt.west, slopeWest) rm(alt.east,alt.west); gc() 3.4 Prepare climate rasters # list chelsa files # CHELSA files are not uploaded to GitHub as they are extremely large and can be downloaded from https://chelsa-climate.org/ # Please note that we downloaded four rasters corresponding to minimum and maximum temperatures for the months of January and June chelsaFiles &lt;- list.files(&quot;data/chelsa/&quot;, full.names = TRUE, pattern = &quot;*.tif&quot;) # gather chelsa data over the east chelsaEast &lt;- purrr::map(chelsaFiles, function(chr){ a &lt;- raster(chr) crs(a) &lt;- crs(elevEast) a &lt;- crop(a, as(eastHim, &quot;Spatial&quot;)) return(a) }) # gather chelsa data over the west chelsaWest &lt;- purrr::map(chelsaFiles, function(chr){ a &lt;- raster(chr) crs(a) &lt;- crs(elevWest) a &lt;- crop(a, as(westHim, &quot;Spatial&quot;)) return(a) }) # Divide temperature values by 10 for east chelsaEast[[1]] &lt;- chelsaEast[[1]]/10 chelsaEast[[2]] &lt;- chelsaEast[[2]]/10 chelsaEast[[3]] &lt;- chelsaEast[[3]]/10 chelsaEast[[4]] &lt;- chelsaEast[[4]]/10 # Divide temperature values by 10 for the west chelsaWest[[1]] &lt;- chelsaWest[[1]]/10 chelsaWest[[2]] &lt;- chelsaWest[[2]]/10 chelsaWest[[3]] &lt;- chelsaWest[[3]]/10 chelsaWest[[4]] &lt;- chelsaWest[[4]]/10 # stack chelsa data for the east and west chelsaEast &lt;- raster::stack(chelsaEast) chelsaWest &lt;- raster::stack(chelsaWest) ### Stack prepared rasters # stack rasters for efficient reprojection later env_east &lt;- stack(elevEast, chelsaEast) env_west &lt;- stack(elevWest, chelsaWest) # get proper names elev_names &lt;- c(&quot;elev&quot;, &quot;slope&quot;, &quot;aspect&quot;) chelsa_names &lt;- c(&quot;maxTemp_Jan&quot;, &quot;maxTemp_June&quot;,&quot;minTemp_Jan&quot;,&quot;minTemp_June&quot;) names(env_east) &lt;- as.character(glue(&#39;{c(elev_names, chelsa_names)}&#39;)) names(env_west) &lt;- as.character(glue(&#39;{c(elev_names, chelsa_names)}&#39;)) 3.5 Extracting data across elevational bands # make duplicate stack envEast &lt;- env_east[[c(&quot;elev&quot;, chelsa_names)]] envWest &lt;- env_West[[c(&quot;elev&quot;, chelsa_names)]] # convert to list envEast &lt;- as.list(envEast) envWest &lt;- as.list(envWest) # map get values over the stack envEast &lt;- purrr::map(envEast, getValues) envWest &lt;- purrr::map(envWest, getValues) names(envEast) &lt;- c(&quot;elev&quot;, chelsa_names) names(envWest) &lt;- c(&quot;elev&quot;, chelsa_names) # convert to dataframe and round to a particular elevational band you need envEast &lt;- bind_cols(envEast) envWest &lt;- bind_cols(envWest) envEast &lt;- drop_na(envEast) %&gt;% mutate(elev_round = plyr::round_any(elev, 100)) %&gt;% # changed to 100 m intervals dplyr::select(-elev) %&gt;% group_by(elev_round) %&gt;% summarise_all(.funs = list(~mean(.), ~ci(.))) %&gt;% mutate(tempRange_Jan = (maxTemp_Jan_mean - minTemp_Jan_mean), tempRange_June = (maxTemp_June_mean - minTemp_June_mean)) envWest &lt;- drop_na(envWest) %&gt;% mutate(elev_round = plyr::round_any(elev, 100)) %&gt;% # changed to 100 m intervals dplyr::select(-elev) %&gt;% group_by(elev_round) %&gt;% summarise_all(.funs = list(~mean(.), ~ci(.))) %&gt;% mutate(tempRange_Jan = (maxTemp_Jan_mean - minTemp_Jan_mean), tempRange_June = (maxTemp_June_mean - minTemp_June_mean)) # Write results to a .csv west_data &lt;- write.csv(env,&quot;results/westHim_100.csv&quot;, row.names = F) east_data &lt;- write.csv(env,&quot;results/eastHim_100.csv&quot;, row.names = F) 3.6 Plot temperature as a function of elevation # eastern Himalayas fig_climate_elevEast &lt;- ggplot(envEast)+ geom_line(aes(x = elev_round, y = mean), size = 0.2, col = &quot;grey&quot;)+ geom_pointrange(aes(x = elev_round, y = mean, ymin=mean-ci, ymax=mean+ci), size = 0.3)+ scale_x_continuous(labels = scales::comma)+ scale_y_continuous(labels = scales::comma)+ facet_wrap(~clim_var, scales = &quot;free_y&quot;)+ theme_few()+ labs(x = &quot;elevation (m) at 100m intervals&quot;, y = &quot;CHELSA variable value&quot;)+ theme(axis.title = element_text(size = 16, face = &quot;bold&quot;), axis.ticks.length.x = unit(.5, &quot;cm&quot;), axis.text = element_text(size = 14), legend.title = element_blank(), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(size = 12)) # save ggplots accordingly ggsave(fig_climate_elevEast, filename = &quot;figs/fig_eastHim_elev100.png&quot;, height = 10, width = 14, device = png(), dpi = 300, units=&quot;in&quot;); dev.off() # western Himalayas fig_climate_elevWest &lt;- ggplot(envWest)+ geom_line(aes(x = elev_round, y = mean), size = 0.2, col = &quot;grey&quot;)+ geom_pointrange(aes(x = elev_round, y = mean, ymin=mean-ci, ymax=mean+ci), size = 0.3)+ scale_x_continuous(labels = scales::comma)+ scale_y_continuous(labels = scales::comma)+ facet_wrap(~clim_var, scales = &quot;free_y&quot;)+ theme_few()+ labs(x = &quot;elevation (m) at 100m intervals&quot;, y = &quot;CHELSA variable value&quot;)+ theme(axis.title = element_text(size = 16, face = &quot;bold&quot;), axis.ticks.length.x = unit(.5, &quot;cm&quot;), axis.text = element_text(size = 14), legend.title = element_blank(), legend.key.size = unit(1,&quot;cm&quot;), legend.text = element_text(size = 12)) # save ggplots accordingly ggsave(fig_climate_elevWest, filename = &quot;figs/fig_westHim_elev100.png&quot;, height = 10, width = 14, device = png(), dpi = 300, units=&quot;in&quot;); dev.off() "],["processing-ebird-data.html", "Section 4 Processing eBird data 4.1 Load necessary libraries 4.2 Loading custom functions to process eBird data 4.3 Use the function written above to extract eBird data 4.4 Extract elevation at unique locations", " Section 4 Processing eBird data In this script, we will process the community science data across the Eastern and Western Himalayas. 4.1 Load necessary libraries library(tidyverse) library(sf) library(raster) library(parallel) 4.2 Loading custom functions to process eBird data # This function processes the ebird data as long as the path where the data is stored and list of countries are mentioned readcleanrawdata = function(rawpath, Country) { require(lubridate) require(tidyverse) require(cowplot) preimp = c(&quot;COMMON.NAME&quot;,&quot;OBSERVATION.COUNT&quot;, &quot;LOCALITY.ID&quot;,&quot;LOCALITY.TYPE&quot;, &quot;STATE&quot;, &quot;COUNTRY&quot;, &quot;LATITUDE&quot;,&quot;LONGITUDE&quot;,&quot;OBSERVATION.DATE&quot;,&quot;TIME.OBSERVATIONS.STARTED&quot;,&quot;OBSERVER.ID&quot;, &quot;PROTOCOL.TYPE&quot;,&quot;DURATION.MINUTES&quot;,&quot;EFFORT.DISTANCE.KM&quot;, &quot;REVIEWED&quot;, &quot;NUMBER.OBSERVERS&quot;,&quot;ALL.SPECIES.REPORTED&quot;,&quot;GROUP.IDENTIFIER&quot;,&quot;SAMPLING.EVENT.IDENTIFIER&quot;,&quot;APPROVED&quot;,&quot;CATEGORY&quot;) nms = read.delim(rawpath, nrows = 1, sep = &quot;\\t&quot;, header = T, quote = &quot;&quot;, stringsAsFactors = F, na.strings = c(&quot;&quot;,&quot; &quot;,NA)) nms = names(nms) nms[!(nms %in% preimp)] = &quot;NULL&quot; nms[nms %in% preimp] = NA data = read.delim(rawpath, colClasses = nms, sep = &quot;\\t&quot;, header = T, quote = &quot;&quot;, stringsAsFactors = F, na.strings = c(&quot;&quot;,&quot; &quot;,NA)) ## choosing important variables imp = c(&quot;COMMON.NAME&quot;,&quot;OBSERVATION.COUNT&quot;, &quot;LOCALITY.ID&quot;,&quot;LOCALITY.TYPE&quot;, &quot;STATE&quot;, &quot;COUNTRY&quot;, &quot;LATITUDE&quot;,&quot;LONGITUDE&quot;,&quot;OBSERVATION.DATE&quot;,&quot;TIME.OBSERVATIONS.STARTED&quot;,&quot;OBSERVER.ID&quot;, &quot;PROTOCOL.TYPE&quot;,&quot;DURATION.MINUTES&quot;,&quot;EFFORT.DISTANCE.KM&quot;, &quot;SAMPLING.EVENT.IDENTIFIER&quot;, &quot;NUMBER.OBSERVERS&quot;,&quot;ALL.SPECIES.REPORTED&quot;,&quot;group.id&quot;, &quot;CATEGORY&quot;,&quot;no.sp&quot;) days = c(31,28,31,30,31,30,31,31,30,31,30,31) cdays = c(0,31,59,90,120,151,181,212,243,273,304,334) ## setup eBird data ## ## filter approved observations, species, slice by single group ID, remove repetitions ## remove repeats ## set date, add month, year and day columns using package LUBRIDATE ## filter distance travelled, duration birded and number of observers ## add number of species column (no.sp) if (Country != &quot;India&quot;) { data = data %&gt;% filter(REVIEWED == 0 | APPROVED == 1) %&gt;% mutate(group.id = ifelse(is.na(GROUP.IDENTIFIER), SAMPLING.EVENT.IDENTIFIER, GROUP.IDENTIFIER)) %&gt;% filter(ALL.SPECIES.REPORTED == 1) %&gt;% filter(EFFORT.DISTANCE.KM&lt;=2.5|is.na(EFFORT.DISTANCE.KM))%&gt;% filter(DURATION.MINUTES &lt;= 120)%&gt;% filter(NUMBER.OBSERVERS &lt;= 10)%&gt;% group_by(group.id,COMMON.NAME) %&gt;% slice(1) %&gt;% ungroup %&gt;% group_by(group.id) %&gt;% mutate(no.sp = n_distinct(COMMON.NAME))%&gt;% dplyr::select(imp) %&gt;% mutate(OBSERVATION.DATE = as.Date(OBSERVATION.DATE), month = month(OBSERVATION.DATE), year = year(OBSERVATION.DATE), day = day(OBSERVATION.DATE) + cdays[month], week = week(OBSERVATION.DATE), fort = ceiling(day/14)) %&gt;% ungroup return(data) } if (Country == &quot;India&quot;) { data = data %&gt;% filter(REVIEWED == 0 | APPROVED == 1) %&gt;% mutate(group.id = ifelse(is.na(GROUP.IDENTIFIER), SAMPLING.EVENT.IDENTIFIER, GROUP.IDENTIFIER)) %&gt;% filter(ALL.SPECIES.REPORTED == 1) %&gt;% filter(EFFORT.DISTANCE.KM&lt;=2.5|is.na(EFFORT.DISTANCE.KM))%&gt;% filter(DURATION.MINUTES &lt;= 120)%&gt;% filter(NUMBER.OBSERVERS &lt;= 10)%&gt;% filter(STATE == &quot;Uttarakhand&quot; | STATE == &quot;Himachal Pradesh&quot;| STATE == &quot;Jammu and Kashmir&quot;| STATE == &quot;Sikkim&quot; | STATE == &quot;Arunachal Pradesh&quot; |STATE == &quot;West Bengal&quot;)%&gt;% group_by(group.id,COMMON.NAME) %&gt;% slice(1) %&gt;% ungroup %&gt;% group_by(group.id) %&gt;% mutate(no.sp = n_distinct(COMMON.NAME))%&gt;% dplyr::select(imp) %&gt;% mutate(OBSERVATION.DATE = as.Date(OBSERVATION.DATE), month = month(OBSERVATION.DATE), year = year(OBSERVATION.DATE), day = day(OBSERVATION.DATE) + cdays[month], week = week(OBSERVATION.DATE), fort = ceiling(day/14)) %&gt;% ungroup return(data) } } 4.3 Use the function written above to extract eBird data # please download the latest versions of eBird data from https://ebird.org/data/download and set the file path accordingly. Since these two datasets are extremely large, we have not uploaded the same to github. # In this study, the latest version of the data corresponds to July 31st 2021 # extract data for the following list of countries Bhutan &lt;-readcleanrawdata(&quot;ebd_BT_relJul-2021.txt&quot;, Country = &quot;Bhutan&quot;) Nepal &lt;-readcleanrawdata(&quot;ebd_NP_relJul-2021.txt&quot;, Country = &quot;Nepal&quot;) POK &lt;-readcleanrawdata(&quot;ebd_IN-JK-KM_relJul-2021.txt&quot;, Country = &quot;POK&quot;) India &lt;-readcleanrawdata(&quot;ebd_IN_relJul-2021.txt&quot;, Country = &quot;India&quot;) Pakistan &lt;-readcleanrawdata(&quot;ebd_PK_relJul-2021.txt&quot;, Country = &quot;Pakistan&quot;) ## Removing non himalayan regions India &lt;-India %&gt;% filter(LATITUDE&gt;26,LONGITUDE&lt;100) Pakistan &lt;-Pakistan %&gt;% filter (LATITUDE&gt;33) dat &lt;-rbind(Nepal,POK,India,Pakistan,Bhutan) # Keep only unique locations used dat &lt;- dat %&gt;% distinct(LATITUDE,LONGITUDE, .keep_all = T) %&gt;% select(LOCALITY.ID,LATITUDE,LONGITUDE) write.csv(dat, &quot;results/unique-loc.csv&quot;, row.names = F) 4.4 Extract elevation at unique locations dat &lt;- st_as_sf(dat, coords = c(&quot;LONGITUDE&quot;,&quot;LATITUDE&quot;), crs=4326, remove = &quot;F&quot;) # Loading the elevation data elev &lt;- raster(&quot;data/elevation/alt&quot;) # extract elevation elevDat &lt;- raster::extract(elev,dat) # cbind elevation back to dataframe dat &lt;- cbind(dat,elevDat) # save Rdata file (uploaded to GitHub) save(dat, file = &quot;results/eBird_elev.RData&quot;) "],["resampling-analysis.html", "Section 5 Resampling analysis 5.1 Load .Rdata file containing elevation data across eBird sampling locations 5.2 Create resampled datasets", " Section 5 Resampling analysis Using the previously generated .Rdata file, we resample checklists for three levels of sampling effort at different elevational bands. 5.1 Load .Rdata file containing elevation data across eBird sampling locations load(&quot;results/eBird_elev.RData&quot;) dat &lt;- as.data.frame(dat) dat &lt;- dat [,-27] # removing unnecessary columns ## include only full species dat &lt;- dat %&gt;% filter(CATEGORY == &quot;species&quot; | CATEGORY == &quot;issf&quot;) # dividing eastern and western himalayas by the 83E longitude datWest &lt;- dat %&gt;% filter (LONGITUDE &lt; 83) datEast &lt;- dat %&gt;% filter (LONGITUDE &gt; 83) 5.2 Create resampled datasets # Creating resampled Dataset for the centre, lower, and upper limit of a species&#39; elevational distribution for 3 levels of sampling effort (number of checklists)- separately for east and west ## Eastern Himalayas ## Number of checklists in either season in each elevational band Checklists &lt;- datEast[!duplicated(datEast$group.id), ] Checklists$elev_level &lt;- cut(Checklists$elevation, breaks = c(-Inf, 500, 1000, 1500, 2000, 2500, 3000, Inf), labels = 1:7) Checklists.S &lt;- subset(Checklists, month %in% 3:7) # summer Checklists.W &lt;- subset(Checklists, month %in% c(1, 2, 11, 12)) # winter summer &lt;- Checklists.S %&gt;% group_by(elev_level) %&gt;% summarise(summer = n_distinct(group.id)) winter &lt;- Checklists.W %&gt;% group_by(elev_level) %&gt;% summarise(winter = n_distinct(group.id)) ## if you want to output a table the number of checklists at each elevation band and season CL_ES &lt;- left_join(summer, winter, by = &quot;elev_level&quot;) levels(CL_ES$elev_level) &lt;- c(&quot;0-500&quot;,&quot;500-1000&quot;,&quot;1000-1500&quot;,&quot;1500-2000&quot;,&quot;2000-2500&quot;,&quot;2500-3000&quot;,&quot;&gt;3000&quot;) write.csv(CL_ES, &quot;ChecklistNo_SeasonElevation_East.csv&quot;, row.names = F) ## Sampling event IDs in each season and in each elevation band ID.S &lt;- lapply(1:7, function(x) Checklists.S$group.id[Checklists.S$elev_level == x]) ID.W &lt;- lapply(1:7, function(x) Checklists.W$group.id[Checklists.W$elev_level == x]) ## Get unique species list uniSpe &lt;- datEast %&gt;% filter(CATEGORY == &quot;species&quot; | CATEGORY == &quot;issf&quot;) uniSpe &lt;- unique(uniSpe[, &quot;COMMON.NAME&quot;]) %&gt;% data.frame() ## Get first second and third quartile of effort (number of checklists) across season and elevation efforts.S &lt;- summary(Checklists.S$elev_level) efforts.W &lt;- summary(Checklists.W$elev_level) qEffort &lt;- quantile(c(efforts.S, efforts.W), c(0.25,0.50,0.75)) # resample for the lower limit (5th percentile), center (median), and upper limit (95th percentile) for a species elevational distribution for (qt in c(0.05, 0.50, 0.95)) { #resample for the levels of sampling effort for (i in 1:3) { ## sample an equal number of checklists (3 levels of effort) from each elevation band set.seed(56789) sampleID.S &lt;- lapply(1:1000, function(y) {unlist(lapply(1:7, function(x) sample(ID.S[[x]], qEffort[i], replace=T)))}) set.seed(56789) sampleID.W &lt;- lapply(1:1000, function(y) {unlist(lapply(1:7, function(x) sample(ID.W[[x]], qEffort[i], replace=T)))}) # calculate the lower, median and upper elevation distribution for each bird species in the two seasons # This step takes awhile # We used parallel processing rs&lt;- mclapply(1:1000, function(y) { m &lt;- t(sapply(uniSpe[, 1], function(x){ occs.S &lt;- subset(datEast, COMMON.NAME ==x &amp; group.id %in% sampleID.S[[y]], select=&quot;elevation&quot;) occs.W &lt;- subset(datEast, COMMON.NAME==x &amp; group.id %in% sampleID.W[[y]], select=&quot;elevation&quot;) B.n &lt;- nrow(occs.S) W.n &lt;- nrow(occs.W) B.elev &lt;- if (B.n &gt; 0) quantile(occs.S$elevation, qt) else NA W.elev &lt;- if (W.n &gt; 0) quantile(occs.W$elevation, qt) else NA return(c(B.elev = B.elev, B.n = B.n, W.elev = W.elev, W.n = W.n)) })) }, mc.cores = 15) save(rs,file = paste0(&quot;eBird_resampled_east_&quot;, sprintf(&quot;%02d&quot;, qt*100),&quot;.q&quot;, i ,&quot;.RData&quot;)) } } ## Repeat the above process for Westenr Himalayas Checklists &lt;- datWest[!duplicated(datWest$group.id), ] Checklists$elev_level &lt;- cut(Checklists$elevation, breaks = c(-Inf, 500, 1000, 1500, 2000, 2500, 3000, Inf), labels = 1:7) Checklists.S &lt;- subset(Checklists, month %in% 3:7) Checklists.W &lt;- subset(Checklists, month %in% c(1, 2, 11, 12)) summer &lt;- Checklists.S %&gt;% group_by(elev_level) %&gt;% summarise(summer = n_distinct(group.id)) winter &lt;- Checklists.W %&gt;% group_by(elev_level) %&gt;% summarise(winter = n_distinct(group.id)) CL_ES &lt;- left_join(summer,winter, by = &quot;elev_level&quot;) levels(CL_ES$elev_level) &lt;-c(&quot;0-500&quot;,&quot;500-1000&quot;,&quot;1000-1500&quot;,&quot;1500-2000&quot;,&quot;2000-2500&quot;,&quot;2500-3000&quot;,&quot;&gt;3000&quot;) write.csv(CL_ES, &quot;ChecklistNo_SeasonElevation_West.csv&quot;, row.names = F ) ID.S &lt;- lapply(1:7, function(x) Checklists.S$group.id[Checklists.S$elev_level == x]) ID.W &lt;- lapply(1:7, function(x) Checklists.W$group.id[Checklists.W$elev_level == x]) uniSpe &lt;- datWest %&gt;% filter(CATEGORY == &quot;species&quot; | CATEGORY == &quot;issf&quot;) uniSpe &lt;- unique(uniSpe[, &quot;COMMON.NAME&quot;]) %&gt;% data.frame() efforts.S &lt;- summary(Checklists.S$elev_level) efforts.W &lt;- summary(Checklists.W$elev_level) qEffort &lt;- quantile(c(efforts.S, efforts.W), c(0.25,0.50,0.75)) for (qt in c(0.05, 0.50, 0.95)) { for (i in 1:3) { set.seed(56789) sampleID.S &lt;- lapply(1:1000, function(y) {unlist(lapply(1:7, function(x) sample(ID.S[[x]], qEffort[i], replace=T)))}) set.seed(56789) sampleID.W &lt;- lapply(1:1000, function(y) {unlist(lapply(1:7, function(x) sample(ID.W[[x]], qEffort[i], replace=T)))}) rs&lt;- mclapply(1:1000, function(y) { m &lt;- t(sapply(uniSpe[, 1], function(x){ occs.S &lt;- subset(datWest, COMMON.NAME ==x &amp; group.id %in% sampleID.S[[y]], select=&quot;elevation&quot;) occs.W &lt;- subset(datWest, COMMON.NAME==x &amp; group.id %in% sampleID.W[[y]], select=&quot;elevation&quot;) B.n &lt;- nrow(occs.S) W.n &lt;- nrow(occs.W) B.elev &lt;- if (B.n &gt; 0) quantile(occs.S$elevation, qt) else NA W.elev &lt;- if (W.n &gt; 0) quantile(occs.W$elevation, qt) else NA return(c(B.elev = B.elev, B.n = B.n, W.elev = W.elev, W.n = W.n)) })) }, mc.cores = 15 ) save(rs,file = paste0(&quot;eBird_resampled_west_&quot;, sprintf(&quot;%02d&quot;, qt*100),&quot;.q&quot;, i,&quot;.RData&quot;)) } } "],["extent-of-elevational-migration.html", "Section 6 Extent of elevational migration 6.1 Load necessary libraries 6.2 Eastern Himalayas 6.3 Western Himalayas", " Section 6 Extent of elevational migration In this script, we calculate the median migration extent, breeding and wintering elevation for all combinations of elevayional limit and sampling effort-seperately for the eastern and western Himalayas 6.1 Load necessary libraries library(ape) library(geiger) library(phytools) library(coxme) library(evobiR) library(nlme) library(tidyverse) library(phangorn) 6.2 Eastern Himalayas # load previously generated .Rdata files for analyses for (minSample in c(30, 60)) { for (ds in c(&quot;05.q1&quot;, &quot;05.q2&quot;, &quot;05.q3&quot;, &quot;95.q1&quot;, &quot;95.q2&quot;, &quot;95.q3&quot;, &quot;50.q1&quot;, &quot;50.q2&quot;, &quot;50.q3&quot;)) { load(paste(&quot;eBird_resampled_east_&quot;, ds, &quot;.Rdata&quot;, sep = &quot;&quot;)) B.elev &lt;- sapply(1:1000, function(x) rs[[x]][, 1]) B.n &lt;- sapply(1:1000, function(x) rs[[x]][, 2]) W.elev &lt;- sapply(1:1000, function(x) rs[[x]][, 3]) W.n &lt;- sapply(1:1000, function(x) rs[[x]][, 4]) diff &lt;- B.elev - W.elev dimnames(diff)&lt;-list(uniSpe[,1],1:1000) # For each species exclude trials where it has been detected less than 30/60 times in winter or summer if (minSample == 30) diff[W.n &lt; 30 | B.n &lt; 30] &lt;- NA if (minSample == 60) diff[W.n &lt; 60 | B.n &lt; 60] &lt;- NA diff.sel &lt;- diff[apply(diff, 1, FUN = function(x) sum(is.na(x))) &lt; 1000, ] # Calculate medians of the percentiles over 1000 sets of resampled sampling events. Along with confidence intervals med.CI &lt;- apply(diff.sel, 1, FUN = function(x) quantile((x), c(0.025, .5, .975), na.rm = TRUE)) med.CI &lt;- t(med.CI) med.CI &lt;- as.data.frame(med.CI) med.CI$Group &lt;- with(med.CI, ifelse(`2.5%` &gt; 0 &amp; `97.5%` &gt; 0, 2, ifelse(`2.5%` &lt; 0 &amp; `97.5%` &lt; 0, 1, 0))) med.CI$LCI_diff &lt;- as.numeric(med.CI$`2.5%`) med.CI$Median_diff &lt;- as.numeric(med.CI$`50%`) med.CI$UCI_diff &lt;- as.numeric(med.CI$`97.5%`) med.CI$Species &lt;- rownames(med.CI) med.CI&lt;-med.CI[,-c(1,2,3)] ## adding breeding elevations to the same table dimnames(B.elev)&lt;-list(uniSpe[,1],1:1000) if (minSample == 30) B.elev[W.n &lt; 30 | B.n &lt; 30] &lt;- NA if (minSample == 60) B.elev[W.n &lt; 60 | B.n &lt; 60] &lt;- NA B.sel &lt;- B.elev[apply(B.elev, 1, FUN = function(x) sum(is.na(x))) &lt; 1000, ] med.S.CI &lt;- apply(B.sel, 1, FUN = function(x) quantile((x), c(0.025, .5, .975), na.rm = TRUE)) med.S.CI &lt;- t(med.S.CI) med.S.CI &lt;- as.data.frame(med.S.CI) med.S.CI$LCI_S &lt;- as.numeric(med.S.CI$`2.5%`) med.S.CI$Median_S &lt;- as.numeric(med.S.CI$`50%`) med.S.CI$UCI_S &lt;- as.numeric(med.S.CI$`97.5%`) med.S.CI$Species &lt;- rownames(med.S.CI) med.S.CI&lt;-med.S.CI[,-c(1,2,3)] ## adding winter elevations to the same table dimnames(W.elev)&lt;-list(uniSpe[,1],1:1000) if (minSample == 30) W.elev[W.n &lt; 30 | B.n &lt; 30] &lt;- NA if (minSample == 60) W.elev[W.n &lt; 60 | B.n &lt; 60] &lt;- NA W.sel &lt;- W.elev[apply(W.elev, 1, FUN = function(x) sum(is.na(x))) &lt; 1000, ] med.W.CI &lt;- apply(W.sel, 1, FUN = function(x) quantile((x), c(0.025, .5, .975), na.rm = TRUE)) med.W.CI &lt;- t(med.W.CI) med.W.CI &lt;- as.data.frame(med.W.CI) med.W.CI$LCI_W &lt;- as.numeric(med.W.CI$`2.5%`) med.W.CI$Median_W &lt;- as.numeric(med.W.CI$`50%`) med.W.CI$UCI_W &lt;- as.numeric(med.W.CI$`97.5%`) med.W.CI$Species &lt;- rownames(med.W.CI) med.W.CI&lt;-med.W.CI[,-c(1,2,3)] ts&lt;-left_join(med.CI,med.S.CI, by = &quot;Species&quot;) %&gt;% left_join(., med.W.CI, by = &quot;Species&quot;, ) ts &lt;- ts[c(&quot;Species&quot;, &quot;Group&quot;, &quot;Median_S&quot;, &quot;LCI_S&quot;, &quot;UCI_S&quot;,&quot;Median_W&quot;,&quot;LCI_W&quot;,&quot;UCI_W&quot;,&quot;Median_diff&quot;,&quot;LCI_diff&quot;, &quot;UCI_diff&quot;)] write.csv(ts, file = paste(&quot;birdlist_east_&quot;,ds,&quot;.sam&quot;,minSample, &quot;.csv&quot;, sep = &quot;&quot;), row.names = F) } } 6.3 Western Himalayas for (minSample in c(30, 60)) { for (ds in c(&quot;05.q1&quot;, &quot;05.q2&quot;, &quot;05.q3&quot;, &quot;95.q1&quot;, &quot;95.q2&quot;, &quot;95.q3&quot;, &quot;50.q1&quot;, &quot;50.q2&quot;, &quot;50.q3&quot;)) { load(paste(&quot;eBird_resampled_west_&quot;, ds, &quot;.Rdata&quot;, sep = &quot;&quot;)) B.elev &lt;- sapply(1:1000, function(x) rs[[x]][, 1]) B.n &lt;- sapply(1:1000, function(x) rs[[x]][, 2]) W.elev &lt;- sapply(1:1000, function(x) rs[[x]][, 3]) W.n &lt;- sapply(1:1000, function(x) rs[[x]][, 4]) diff &lt;- B.elev - W.elev dimnames(diff)&lt;-list(uniSpe[,1],1:1000) # For each species exclude trials where it has been detected less than 30/60 times in winter or summer if (minSample == 30) diff[W.n &lt; 30 | B.n &lt; 30] &lt;- NA if (minSample == 60) diff[W.n &lt; 60 | B.n &lt; 60] &lt;- NA diff.sel &lt;- diff[apply(diff, 1, FUN = function(x) sum(is.na(x))) &lt; 1000, ] # Calculate medians of the percentiles over 1000 sets of resampled sampling events. Along with confidence intervals med.CI &lt;- apply(diff.sel, 1, FUN = function(x) quantile((x), c(0.025, .5, .975), na.rm = TRUE)) med.CI &lt;- t(med.CI) med.CI &lt;- as.data.frame(med.CI) med.CI$Group &lt;- with(med.CI, ifelse(`2.5%` &gt; 0 &amp; `97.5%` &gt; 0, 2, ifelse(`2.5%` &lt; 0 &amp; `97.5%` &lt; 0, 1, 0))) med.CI$LCI_diff &lt;- as.numeric(med.CI$`2.5%`) med.CI$Median_diff &lt;- as.numeric(med.CI$`50%`) med.CI$UCI_diff &lt;- as.numeric(med.CI$`97.5%`) med.CI$Species &lt;- rownames(med.CI) med.CI&lt;-med.CI[,-c(1,2,3)] ## adding breeding elevations to the same table dimnames(B.elev)&lt;-list(uniSpe[,1],1:1000) if (minSample == 30) B.elev[W.n &lt; 30 | B.n &lt; 30] &lt;- NA if (minSample == 60) B.elev[W.n &lt; 60 | B.n &lt; 60] &lt;- NA B.sel &lt;- B.elev[apply(B.elev, 1, FUN = function(x) sum(is.na(x))) &lt; 1000, ] med.S.CI &lt;- apply(B.sel, 1, FUN = function(x) quantile((x), c(0.025, .5, .975), na.rm = TRUE)) med.S.CI &lt;- t(med.S.CI) med.S.CI &lt;- as.data.frame(med.S.CI) med.S.CI$LCI_S &lt;- as.numeric(med.S.CI$`2.5%`) med.S.CI$Median_S &lt;- as.numeric(med.S.CI$`50%`) med.S.CI$UCI_S &lt;- as.numeric(med.S.CI$`97.5%`) med.S.CI$Species &lt;- rownames(med.S.CI) med.S.CI&lt;-med.S.CI[,-c(1,2,3)] ## adding winter elevations to the same table dimnames(W.elev)&lt;-list(uniSpe[,1],1:1000) if (minSample == 30) W.elev[W.n &lt; 30 | B.n &lt; 30] &lt;- NA if (minSample == 60) W.elev[W.n &lt; 60 | B.n &lt; 60] &lt;- NA W.sel &lt;- W.elev[apply(W.elev, 1, FUN = function(x) sum(is.na(x))) &lt; 1000, ] med.W.CI &lt;- apply(W.sel, 1, FUN = function(x) quantile((x), c(0.025, .5, .975), na.rm = TRUE)) med.W.CI &lt;- t(med.W.CI) med.W.CI &lt;- as.data.frame(med.W.CI) med.W.CI$LCI_W &lt;- as.numeric(med.W.CI$`2.5%`) med.W.CI$Median_W &lt;- as.numeric(med.W.CI$`50%`) med.W.CI$UCI_W &lt;- as.numeric(med.W.CI$`97.5%`) med.W.CI$Species &lt;- rownames(med.W.CI) med.W.CI&lt;-med.W.CI[,-c(1,2,3)] ts&lt;-left_join(med.CI,med.S.CI, by = &quot;Species&quot;) %&gt;% left_join(., med.W.CI, by = &quot;Species&quot;, ) ts &lt;- ts[c(&quot;Species&quot;, &quot;Group&quot;, &quot;Median_S&quot;, &quot;LCI_S&quot;, &quot;UCI_S&quot;,&quot;Median_W&quot;,&quot;LCI_W&quot;,&quot;UCI_W&quot;,&quot;Median_diff&quot;,&quot;LCI_diff&quot;, &quot;UCI_diff&quot;)] write.csv(ts, file = paste(&quot;birdlist_west&quot;,ds,&quot;.sam&quot;,minSample, &quot;.csv&quot;, sep = &quot;&quot;), row.names = F) } } Elevational shift in the eastern and western Himalayas Elevational shift in the eastern Himalayas (A) and western Himalayas (B). Figures show species estimates (at high sampling effort) of median summer elevations (orange circles) in descending order and their respective median winter elevations (teal triangles). Errors bars indicate 95% confidence intervals estimated from the resampling protocol. "],["geometric-constaints-pgls.html", "Section 7 Geometric constaints PGLS", " Section 7 Geometric constaints PGLS In this script, we tested for an association between absolute elevational shift (to retain both upslope and downslope shifts) and the breadth of elevational distribution (elevational range) of the species, in which we calculated elevational range by subtracting the summer lower elevation limit from the summer upper elevational limit. To test whether elevational shifts are greater for high or low elevation birds simply because they have more elevation to potentially shift (upslope or downslope), we calculated absolute relative elevational shift as the elevational shift at each elevational limit (upper, median, lower) divided by the predicted elevation for the respective limit (i.e.i.e., elevational shift at upper limit / elevational upper limit). tree &lt;- read.nexus(&quot;data/birdtree.nex&quot;) tree &lt;- mcc(tree) ## maximum clade credibility tree x &lt;- East_ele_range_higheffort ### repeat for East_ele_range_mediumeffort, East_ele_range_loweffort, West_ele_range_mediumeffort, West_ele_range_higheffort,West_ele_range_loweffort x$absrelshift_upper&lt;-abs(x$Diff_ratio_upper) x$absrelshift_median&lt;-abs(x$Diff_ratio_median) x$absrelshift_lower&lt;-abs(x$Diff_ratio_lower) pruned.tree &lt;-drop.tip(tree,tree$tip.label[-match(x$Tree.name, tree$tip.label)]) pruned.tree glsdata&lt;-x[match(pruned.tree$tip.label, x$Tree.name),] pgls1&lt;-gls(absrelshift_upper~Summer_range,correlation=corPagel(1,pruned.tree),data=glsdata) j&lt;-summary(pgls1) j pgls3&lt;-gls(absrelshift_median~Summer_range,correlation=corPagel(1,pruned.tree),data=glsdata) k&lt;-summary(pgls3) k pgls4&lt;-gls(absrelshift_lower~Summer_range,correlation=corPagel(1,pruned.tree),data=glsdata) l&lt;-summary(pgls4) l outputj&lt;-as.data.frame(j$tTable) outputj$Lambda&lt;-j$modelStruct outputj&lt;-as.matrix(outputj) outputk&lt;-as.data.frame(k$tTable) outputk$Lambda&lt;-k$modelStruct outputk&lt;-as.matrix(outputk) outputl&lt;-as.data.frame(l$tTable) outputl$Lambda&lt;-l$modelStruct outputl&lt;-as.matrix(outputl) write.csv(outputj,file=&quot;East_absrelshift_upper~Summer_range_higheffort.csv&quot;,row.names=FALSE) write.csv(outputk,file=&quot;East_absrelshift_median~Summer_range_higheffort.csv&quot;,row.names=FALSE) write.csv(outputl,file=&quot;East_absrelshift_lower~Summer_range_higheffort.csv.csv&quot;,row.names=FALSE) ## Absolute shift ~ Summer Elevational x&lt;-East_ele_range_higheffort ### repeat for East_ele_range_mediumeffort, East_ele_range_loweffort, West_ele_range_mediumeffort, West_ele_range_higheffort,West_ele_range_loweffort x$absrelshift_upper&lt;-abs(x$Diff_ratio_upper) x$absrelshift_median&lt;-abs(x$Diff_ratio_median) x$absrelshift_lower&lt;-abs(x$Diff_ratio_lower) pruned.tree&lt;-drop.tip(tree1,tree1$tip.label[-match(x$Tree.name, tree1$tip.label)]) pruned.tree glsdata&lt;-x[match(pruned.tree$tip.label, x$Tree.name),] pgls1&lt;-gls(absrelshift_upper~Median_S_upper,correlation=corPagel(1,pruned.tree),data=glsdata) j&lt;-summary(pgls1) j pgls3&lt;-gls(absrelshift_median~Median_S_median,correlation=corPagel(1,pruned.tree),data=glsdata) k&lt;-summary(pgls3) k pgls4&lt;-gls(absrelshift_lower~Median_S_lower,correlation=corPagel(1,pruned.tree),data=glsdata) l&lt;-summary(pgls4) l outputj&lt;-as.data.frame(j$tTable) outputj$Lambda&lt;-j$modelStruct outputj&lt;-as.matrix(outputj) outputk&lt;-as.data.frame(k$tTable) outputk$Lambda&lt;-k$modelStruct outputk&lt;-as.matrix(outputk) outputl&lt;-as.data.frame(l$tTable) outputl$Lambda&lt;-l$modelStruct outputl&lt;-as.matrix(outputl) write.csv(outputj,file=&quot;East_absrelshift_upper~Median_S_upper_higheffort.csv&quot;,row.names=FALSE) write.csv(outputk,file=&quot;East_absrelshift_median~Median_S_median_higheffort.csv&quot;,row.names=FALSE) write.csv(outputl,file=&quot;East_absrelshift_lower~Median_S_lower_higheffort.csv.csv&quot;,row.names=FALSE) Geometric constraints do not drive elevational shifts. Dashed lines represent predicted relationship based on linear regression model. Error bands denote standard errors. Blue lines and dots represent downslope shifts while red lines and dots are birds that shift upslope. Top Panel: Relationship between median shift upslope or downslope and summer elevational range in eastern (3A) and western Himalayas (3B), both statistically significant (Supplementary Table 2). Bottom Panel: Relationship between relative elevational shift and summer elevation (median distribution). The relationship was not significant in the east (3C) but significant in the west (3D). "],["phylogenetic-generalized-least-squares-regressions.html", "Section 8 Phylogenetic generalized least squares regressions 8.1 Remove non-Himalayan species 8.2 PGLS models for the Eastern Himalayas 8.3 PGLS models for the Western Himalayas 8.4 Phylogenetic paired t-test", " Section 8 Phylogenetic generalized least squares regressions Here, we used a phylogenetic generalized least squares regression to test if thermal regime, dispersal ability, and diet significantly drive Himalayan bird elevational shift. 8.1 Remove non-Himalayan species # first we remove species that are non-Himalayan rem &lt;-read.csv(&quot;results/non-Himalayan-species.csv&quot;) 8.2 PGLS models for the Eastern Himalayas for (ds in c(&quot;05.q1.sam30&quot;, &quot;05.q1.sam60&quot;, &quot;05.q2.sam30&quot;,&quot;05.q2.sam60&quot;, &quot;05.q3.sam30&quot;,&quot;05.q3.sam60&quot;, &quot;95.q1.sam30&quot;,&quot;95.q1.sam60&quot;, &quot;95.q2.sam30&quot;,&quot;95.q2.sam60&quot; ,&quot;95.q3.sam30&quot;,&quot;95.q3.sam60&quot; ,&quot;50.q1.sam30&quot;,&quot;50.q1.sam60&quot; ,&quot;50.q2.sam30&quot;,&quot;50.q2.sam60&quot; ,&quot;50.q3.sam30&quot;, &quot;50.q3.sam60&quot;)) { x&lt;-read.csv(paste(&quot;birdlist_east&quot;, ds, &quot;.csv&quot;, sep = &quot;&quot;)) x&lt;-x[!(x$Species %in% rem$Species),] write.csv(x,file = paste(&quot;birdlist_east_speciesList&quot;,ds,&quot;.csv&quot;, sep = &quot;&quot;), row.names = F) } # load necessary data for running statistical models east_100 &lt;-read.csv(&quot;results/eastHim_100.csv&quot;) east_100_S &lt;- east_100 %&gt;% select(-contains(&quot;Jan&quot;)) %&gt;% rename(elev_roundS = elev_round) east_100_W &lt;- east_100 %&gt;% select(-contains(&quot;June&quot;))%&gt;% rename(elev_roundW = elev_round) jetzname &lt;-read.csv(&quot;for_PGLS_list.csv&quot;) sheard_trait &lt;- read.csv(&quot;data/species-trait-dat.csv&quot;) %&gt;% select(HWI,Diet,Tree.name) tree &lt;-read.nexus(&quot;data/birdtree.nex&quot;) tree &lt;- mcc(tree) ## maximum clade credibility tree for (ds in c(&quot;05.q1.sam30&quot;, &quot;05.q1.sam60&quot;, &quot;05.q2.sam30&quot;,&quot;05.q2.sam60&quot;, &quot;05.q3.sam30&quot;,&quot;05.q3.sam60&quot;, &quot;95.q1.sam30&quot;,&quot;95.q1.sam60&quot;, &quot;95.q2.sam30&quot;,&quot;95.q2.sam60&quot; ,&quot;95.q3.sam30&quot;,&quot;95.q3.sam60&quot; ,&quot;50.q1.sam30&quot;,&quot;50.q1.sam60&quot; ,&quot;50.q2.sam30&quot;,&quot;50.q2.sam60&quot; ,&quot;50.q3.sam30&quot;, &quot;50.q3.sam60&quot;)) { x&lt;-read.csv(paste(&quot;birdlist_east_speciesList&quot;, ds, &quot;.csv&quot;, sep = &quot;&quot;)) x$elev_roundS&lt;-round(x$Median_S,-2) x$elev_roundW&lt;-round(x$Median_W,-2) x&lt;-left_join(x,east_100_S, by = &quot;elev_roundS&quot;) x&lt;-left_join(x,east_100_W, by = &quot;elev_roundW&quot;) x$ThermalRegime&lt;-x$maxTemp_June_mean-x$minTemp_Jan_mean x&lt;-left_join(x,jetzname, by = &quot;Species&quot;) x&lt;-left_join(x, sheard_trait, by = &quot;Tree.name&quot;)%&gt;% mutate(Diet = fct_recode(Diet, &quot;omnivore&quot; = &quot;nectar&quot;, &quot;omnivore&quot; = &quot;scav&quot;, &quot;seeds&quot;=&quot;plants&quot;)) pruned.tree&lt;-drop.tip(tree1,tree1$tip.label[-match(x$Tree.name, tree1$tip.label)]) glsdata&lt;-x[match(pruned.tree$tip.label, x$Tree.name),] write.csv(glsdata, file = paste(&quot;results/east_ordered&quot;,ds,&quot;.csv&quot;, sep = &quot;&quot;), row.names = F) ## merged and ordered dataset tryCatch({pgls&lt;-gls(Median_diff~Median_S,correlation=corPagel(1,pruned.tree),data=glsdata) pgls2&lt;-gls(Median_diff~ThermalRegime+HWI+Diet,correlation=corPagel(1,pruned.tree),data=glsdata) j&lt;-summary(pgls) k&lt;-summary(pgls2) outputj&lt;-as.data.frame(j$tTable) outputj$Lambda&lt;-j$modelStruct outputj&lt;-as.matrix(outputj) outputk&lt;-as.data.frame(k$tTable) outputk$Lambda&lt;-k$modelStruct outputk&lt;-as.matrix(outputk) write.csv(outputj, file = paste(&quot;results/east_Mod1Out&quot;, ds, &quot;.csv&quot;, sep = &quot;&quot;)) write.csv(outputk, file = paste(&quot;results/east_Mod2Out&quot;, ds, &quot;.csv&quot;, sep = &quot;&quot;))}, error=function(e){}) } 8.3 PGLS models for the Western Himalayas for (ds in c(&quot;05.q1.sam30&quot;, &quot;05.q1.sam60&quot;, &quot;05.q2.sam30&quot;,&quot;05.q2.sam60&quot;, &quot;05.q3.sam30&quot;,&quot;05.q3.sam60&quot;, &quot;95.q1.sam30&quot;,&quot;95.q1.sam60&quot;, &quot;95.q2.sam30&quot;,&quot;95.q2.sam60&quot; ,&quot;95.q3.sam30&quot;,&quot;95.q3.sam60&quot; ,&quot;50.q1.sam30&quot;,&quot;50.q1.sam60&quot; ,&quot;50.q2.sam30&quot;,&quot;50.q2.sam60&quot; ,&quot;50.q3.sam30&quot;, &quot;50.q3.sam60&quot;)) { x&lt;-read.csv(paste(&quot;birdlist_west&quot;, ds, &quot;.csv&quot;, sep = &quot;&quot;)) x&lt;-x[!(x$Species %in% rem$Species),] write.csv(x,file = paste(&quot;birdlist_west_speciesList&quot;,ds,&quot;.csv&quot;, sep = &quot;&quot;), row.names = F) ## final list after removing non himalayan species } west_100 &lt;- read.csv(&quot;results/westHim_100.csv&quot;) west_100_S &lt;- west_100 %&gt;% select(-contains(&quot;Jan&quot;)) %&gt;% rename(elev_roundS = elev_round) west_100_W &lt;- west_100 %&gt;% select(-contains(&quot;June&quot;))%&gt;% rename(elev_roundW = elev_round) for (ds in c(&quot;05.q1.sam30&quot;, &quot;05.q1.sam60&quot;, &quot;05.q2.sam30&quot;,&quot;05.q2.sam60&quot;, &quot;05.q3.sam30&quot;,&quot;05.q3.sam60&quot;, &quot;95.q1.sam30&quot;,&quot;95.q1.sam60&quot;, &quot;95.q2.sam30&quot;,&quot;95.q2.sam60&quot; ,&quot;95.q3.sam30&quot;,&quot;95.q3.sam60&quot; ,&quot;50.q1.sam30&quot;,&quot;50.q1.sam60&quot; ,&quot;50.q2.sam30&quot;,&quot;50.q2.sam60&quot; ,&quot;50.q3.sam30&quot;, &quot;50.q3.sam60&quot;)) { x&lt;-read.csv(paste(&quot;birdlist_west_speciesList&quot;, ds, &quot;.csv&quot;, sep = &quot;&quot;)) x$elev_roundS&lt;-round(x$Median_S,-2) x$elev_roundW&lt;-round(x$Median_W,-2) x&lt;-left_join(x,west_100_S, by = &quot;elev_roundS&quot;) x&lt;-left_join(x,west_100_W, by = &quot;elev_roundW&quot;) x$ThermalRegime&lt;-x$maxTemp_June_mean-x$minTemp_Jan_mean x&lt;-left_join(x,jetzname, by = &quot;Species&quot;) x&lt;-left_join(x, sheard_trait, by = &quot;Tree.name&quot;)%&gt;% mutate(Diet = fct_recode(Diet, &quot;omnivore&quot; = &quot;nectar&quot;, &quot;omnivore&quot; = &quot;scav&quot;, &quot;seeds&quot;=&quot;plants&quot;)) pruned.tree&lt;-drop.tip(tree1,tree1$tip.label[-match(x$Tree.name, tree1$tip.label)]) glsdata&lt;-x[match(pruned.tree$tip.label, x$Tree.name),] write.csv(glsdata, file = paste(&quot;results/west_ordered&quot;,ds,&quot;.csv&quot;, sep = &quot;&quot;), row.names = F) tryCatch({pgls&lt;-gls(Median_diff~Median_S,correlation=corPagel(1,pruned.tree),data=glsdata) pgls2&lt;-gls(Median_diff~ThermalRegime+HWI+Diet,correlation=corPagel(1,pruned.tree),data=glsdata) j&lt;-summary(pgls) k&lt;-summary(pgls2) outputj&lt;-as.data.frame(j$tTable) outputj$Lambda&lt;-j$modelStruct outputj&lt;-as.matrix(outputj) outputk&lt;-as.data.frame(k$tTable) outputk$Lambda&lt;-k$modelStruct outputk&lt;-as.matrix(outputk) write.csv(outputj, file = paste(&quot;results/west_Mod1Out&quot;, ds, &quot;.csv&quot;, sep = &quot;&quot;)) write.csv(outputk, file = paste(&quot;results/west_Mod2Out&quot;, ds, &quot;.csv&quot;, sep = &quot;&quot;))}, error=function(e){}) } 8.4 Phylogenetic paired t-test diff_ttest &lt;-setNames(data.frame(matrix(ncol = 3, nrow = 0)), c(&quot;t&quot;, &quot;df&quot;, &quot;pvalue&quot;)) niche_ttest &lt;-setNames(data.frame(matrix(ncol = 3, nrow = 0)), c(&quot;t&quot;, &quot;df&quot;, &quot;pvalue&quot;)) for (ds in c(&quot;05.q1.sam30&quot;, &quot;05.q1.sam60&quot;, &quot;05.q2.sam30&quot;,&quot;05.q2.sam60&quot;, &quot;05.q3.sam30&quot;,&quot;05.q3.sam60&quot;, &quot;95.q1.sam30&quot;,&quot;95.q1.sam60&quot;, &quot;95.q2.sam30&quot;,&quot;95.q2.sam60&quot; ,&quot;95.q3.sam30&quot;,&quot;95.q3.sam60&quot; ,&quot;50.q1.sam30&quot;,&quot;50.q1.sam60&quot; ,&quot;50.q2.sam30&quot;,&quot;50.q2.sam60&quot; ,&quot;50.q3.sam30&quot;, &quot;50.q3.sam60&quot;)) { west&lt;-read.csv(paste(&quot;results/west_ordered&quot;,ds,&quot;.csv&quot;, sep = &quot;&quot;)) east&lt;-read.csv(paste(&quot;results/east_ordered&quot;,ds,&quot;.csv&quot;, sep = &quot;&quot;)) commonW&lt;-west[(west$Species %in% east$Species),] commonE&lt;-east[(east$Species %in% west$Species),] common&lt;-data.frame(Median_diffW = commonW$Median_diff,Median_diffE = commonE$Median_diff, ThermalRegimeW = commonW$ThermalRegime,ThermalRegimeE = commonE$ThermalRegime ,Tree.name = commonE$Tree.name) pruned.tree&lt;-drop.tip(tree1,tree1$tip.label[-match(common$Tree.name, tree1$tip.label)]) comdata&lt;-common[match(pruned.tree$tip.label, common$Tree.name),] row.names(comdata)&lt;-comdata$Tree.name a&lt;-phyl.pairedttest(pruned.tree,comdata[,c(1,2)]) b&lt;-phyl.pairedttest(pruned.tree,comdata[,c(3,4)]) a_diff_ttest&lt;-data.frame(t = a$t, pvalue = a$P, df = a$df, dataset = ds) b_niche_ttest&lt;-data.frame(t = b$t, pvalue = b$P, df = b$df, dataset = ds) diff_ttest&lt;-rbind(diff_ttest,a_diff_ttest) niche_ttest&lt;-rbind(niche_ttest,b_niche_ttest) write.csv(diff_ttest, &quot;results/diff_ttest.csv&quot;, row.names = F) write.csv(niche_ttest, &quot;results/niche_ttest.csv&quot;, row.names = F) } Determinants of elevational shift Determinants of elevational shift in east Himalayan birds (top panel) and western Himalayan birds (bottom panel). Asterisks denote significant (P&lt;0.05) differences. Dots represent model estimate and errors bars denote standard errors for PGLS models used in the comparative analyses. Color of the dot represents sampling effort (light grey: low, dark grey: medium and black: high). Figure represents PGLS model exploring elevational shift at the lower, medianmedian, and upper limits as driven by life history variables (Thermal Regime (TR), Dispersal ability (HWI) and Diet (INV=invertivores, OMN=omnivores, SED=seed eaters, VRT=vertivores)). Within diet classes, reference level is frugivorous diet. Phylogenetic paired t-test For common species in across the east and the west, A) Seasonal elevational shift is not different in the east (black circles) and the west (blue triangles) (Phylogenetic paired t-test, P&lt;0.001). B) Thermal regime is narrower in the east (black circles) for all species compared to the west (blue triangles) (P&lt;0.001). "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
